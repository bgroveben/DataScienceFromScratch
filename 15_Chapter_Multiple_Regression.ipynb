{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15. Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from linear_algebra import dot, vector_add\n",
    "from statistics import median, standard_deviation\n",
    "from probability import normal_cdf\n",
    "from gradient_descent import minimize_stochastic\n",
    "from simple_linear_regression import total_sum_of_squares\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VP is impressed by your simple regression model, but you know you can do better.  \n",
    "You start by collecting more data: for each user you get data on how many hours he works each day and whether he has a PhD.  \n",
    "You can use this additional data to improve your model.  \n",
    "Accordingly, you hypothesize a linear model with more independent variables:  \n",
    "\n",
    "$\\normalsize \\text{minutes} = \\alpha + \\beta_1 \\text{friends} + \\beta_2 \\text{work hours} + \\beta_3 \\text{PhD} + \\epsilon$  \n",
    "\n",
    "For the PhD category we can use a dummy variable (see Chapter 11) that equals 1 for users *with* a PhD and 0 for users *without* a PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Chapter 14 we fit a model of the form:  \n",
    "\n",
    "$\\Large y_i = \\alpha + \\beta x_i + \\epsilon_i$  \n",
    "\n",
    "Now imagine that each input $\\normalsize x_i$ is not a single number, but is instead a vector of $\\normalsize k$ numbers $\\normalsize \\;{x_i}_1, {x_i}_2, \\ldots, {x_i}_k$.  \n",
    "The multiple regression model assumes that:  \n",
    "\n",
    "$\\Large y_i = \\alpha + \\beta_1{x_i}_1 + \\ldots + \\beta_k{x_i}_k + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple regression the vector of parameters is usually called $\\normalsize \\beta$.  \n",
    "We'll want this to include the constant term as well, which we can achieve by adding a column of ones to our data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "beta = [alpha, beta_1, ..., beta_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_i = [1, x_i1, ..., x_ik]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x_i, beta):\n",
    "    \"\"\" assumes that the first element of each x_i is 1 \"\"\"\n",
    "    return dot(x_i, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, our independent variable `x` will be a list of vectors, each of which looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[1,   # constant term\n",
    " 49,  # number of friends\n",
    " 4,   # work hours per day\n",
    " 0]   # doesn't have a PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the data that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [[1,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Assumptions of the Least Squares Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two further assumptions that are required for this model, as well as our solution, to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption the First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $x$ are [linearly independent](https://en.wikipedia.org/wiki/Linear_independence), meaning that there is no way to write any one as a weighted sum of some of the others.  \n",
    "If this assumption fails, there is no reliable way to estimate `beta`.  \n",
    "To illustrate this in an extreme case, imagine that we have an extra field `num_acquaintances` in our data that, for every user, was exactly equal to `num_friends`.  \n",
    "Then, starting with `beta`, if we add *any* amount to the `num_friends` coefficient and subtract the same amount from the `num_acquaintances` coefficient, the model's predictions will remain unchanged.  \n",
    "This means that there is no way to find *the* coefficient for `num_friends`.  \n",
    "Usually violations of this assumption won't be so obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption the Second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $x$ are all uncorrelated with the errors of $\\normalsize \\epsilon$.  \n",
    "If this fails to be the case, our estimates of `beta` will be systematically wrong.  \n",
    "For example, in Chapter 14, we built a model that predicted that each additional friend was associated with an extra 0.90 daily minutes on the site.  \n",
    "Imagine that it's also the case that:  \n",
    "- people who work more hours spend less time on the site.\n",
    "- people with more friends tend to work more hours.\n",
    "In math terms, imagine that the \"actual\" model is:  \n",
    "\n",
    "$\\large \\text{minutes} = \\alpha + \\beta_1 \\text{friends} + \\beta_2 \\text{work hours} + \\epsilon$  \n",
    "\n",
    "and that work hours and friends are positively correlated.  \n",
    "In that case, when we minimize the errors of the single variable model:  \n",
    "\n",
    "$\\large \\text{minutes} = \\alpha + \\beta_1 \\text{friends} + \\epsilon$.  \n",
    "\n",
    "we will underestimate $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what would happen if we made predictions using the single variable model with the \"actual\" value of $\\beta_1$ (the value that arises from minimizing the errors of what we called the \"actual\" model).  \n",
    "The predictions would tend to be too small for users who work many hours and too large for users who work few hours, because $\\beta_2 > 0$ and we failed to include it.  \n",
    "Because work hours is positively correlated with number of friends, this means that the predictions tend to be too small for users with many friends and too large for users with few friends.  \n",
    "The result of this is that we can reduce the errors (in the single-variable model) by decreasing our estimate of $\\beta_1$, which means that the error-minimizing $\\beta_1$ is smaller than the \"actual\" value.  \n",
    "That is, in this case the single-variable least-squares solution is biased to underestimate $\\beta_1$.  \n",
    "And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we did in the simple linear model, we'll choose `beta` to minimize the sum of squared errors.  \n",
    "Finding an exact solution is not simple to do by hand, which means we'll need to use gradient descent.  \n",
    "We'll start by creating an error function to minimize.  \n",
    "For stochastic gradient descent, we'll just want the squared error corresponding to a single prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(x_i, y_i, beta):\n",
    "    return y_i - predict(x_i, beta)\n",
    "\n",
    "def squared_error(x_i, y_i, beta):\n",
    "    return error(x_i, y_i, beta) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know calculus, you can calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error_gradient(x_i, y_i, beta):\n",
    "    \"\"\" the gradient (with respect to beta) corresponding to the ith squared error term \"\"\"\n",
    "    return [-2 * x_ij * error(x_i, y_i, beta) for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're ready to find the optimal beta using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.625234786488353,\n",
       " 0.9715448288696535,\n",
       " -1.8679272872032218,\n",
       " 0.911456949921445]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_beta(x, y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(squared_error,\n",
    "                               squared_error_gradient,\n",
    "                               x, \n",
    "                               y, \n",
    "                               beta_initial,\n",
    "                               0.001)\n",
    "random.seed(0)\n",
    "beta = estimate_beta(x, daily_minutes_good)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results means that our model looks like:  \n",
    "\n",
    "minutes = 30.63 + 0.972friends - 1.868work hours + 0.911PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
