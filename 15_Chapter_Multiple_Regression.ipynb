{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15. Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from linear_algebra import dot, vector_add\n",
    "from statistics import median, standard_deviation\n",
    "from probability import normal_cdf\n",
    "from gradient_descent import minimize_stochastic\n",
    "from simple_linear_regression import total_sum_of_squares\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VP is impressed by your simple regression model, but you know you can do better.  \n",
    "You start by collecting more data: for each user you get data on how many hours he works each day and whether he has a PhD.  \n",
    "You can use this additional data to improve your model.  \n",
    "Accordingly, you hypothesize a linear model with more independent variables:  \n",
    "\n",
    "$\\normalsize \\text{minutes} = \\alpha + \\beta_1 \\text{friends} + \\beta_2 \\text{work hours} + \\beta_3 \\text{PhD} + \\epsilon$  \n",
    "\n",
    "For the PhD category we can use a dummy variable (see Chapter 11) that equals 1 for users *with* a PhD and 0 for users *without* a PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Chapter 14 we fit a model of the form:  \n",
    "\n",
    "$\\Large y_i = \\alpha + \\beta x_i + \\epsilon_i$  \n",
    "\n",
    "Now imagine that each input $\\normalsize x_i$ is not a single number, but is instead a vector of $\\normalsize k$ numbers $\\normalsize \\;{x_i}_1, {x_i}_2, \\ldots, {x_i}_k$.  \n",
    "The multiple regression model assumes that:  \n",
    "\n",
    "$\\Large y_i = \\alpha + \\beta_1{x_i}_1 + \\ldots + \\beta_k{x_i}_k + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple regression the vector of parameters is usually called $\\normalsize \\beta$.  \n",
    "We'll want this to include the constant term as well, which we can achieve by adding a column of ones to our data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "beta = [alpha, beta_1, ..., beta_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_i = [1, x_i1, ..., x_ik]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x_i, beta):\n",
    "    \"\"\" assumes that the first element of each x_i is 1 \"\"\"\n",
    "    return dot(x_i, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, our independent variable `x` will be a list of vectors, each of which looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[1,   # constant term\n",
    " 49,  # number of friends\n",
    " 4,   # work hours per day\n",
    " 0]   # doesn't have a PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the data that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [[1,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Assumptions of the Least Squares Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two further assumptions that are required for this model, as well as our solution, to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption the First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $x$ are [linearly independent](https://en.wikipedia.org/wiki/Linear_independence), meaning that there is no way to write any one as a weighted sum of some of the others.  \n",
    "If this assumption fails, there is no reliable way to estimate `beta`.  \n",
    "To illustrate this in an extreme case, imagine that we have an extra field `num_acquaintances` in our data that, for every user, was exactly equal to `num_friends`.  \n",
    "Then, starting with `beta`, if we add *any* amount to the `num_friends` coefficient and subtract the same amount from the `num_acquaintances` coefficient, the model's predictions will remain unchanged.  \n",
    "This means that there is no way to find *the* coefficient for `num_friends`.  \n",
    "Usually violations of this assumption won't be so obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption the Second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $x$ are all uncorrelated with the errors of $\\normalsize \\epsilon$.  \n",
    "If this fails to be the case, our estimates of `beta` will be systematically wrong.  \n",
    "For example, in Chapter 14, we built a model that predicted that each additional friend was associated with an extra 0.90 daily minutes on the site.  \n",
    "Imagine that it's also the case that:  \n",
    "- people who work more hours spend less time on the site.\n",
    "- people with more friends tend to work more hours.\n",
    "In math terms, imagine that the \"actual\" model is:  \n",
    "\n",
    "$\\large \\text{minutes} = \\alpha + \\beta_1 \\text{friends} + \\beta_2 \\text{work hours} + \\epsilon$  \n",
    "\n",
    "and that work hours and friends are positively correlated.  \n",
    "In that case, when we minimize the errors of the single variable model:  \n",
    "\n",
    "$\\large \\text{minutes} = \\alpha + \\beta_1 \\text{friends} + \\epsilon$.  \n",
    "\n",
    "we will underestimate $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what would happen if we made predictions using the single variable model with the \"actual\" value of $\\beta_1$ (the value that arises from minimizing the errors of what we called the \"actual\" model).  \n",
    "The predictions would tend to be too small for users who work many hours and too large for users who work few hours, because $\\beta_2 > 0$ and we failed to include it.  \n",
    "Because work hours is positively correlated with number of friends, this means that the predictions tend to be too small for users with many friends and too large for users with few friends.  \n",
    "The result of this is that we can reduce the errors (in the single-variable model) by decreasing our estimate of $\\beta_1$, which means that the error-minimizing $\\beta_1$ is smaller than the \"actual\" value.  \n",
    "That is, in this case the single-variable least-squares solution is biased to underestimate $\\beta_1$.  \n",
    "And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we did in the simple linear model, we'll choose `beta` to minimize the sum of squared errors.  \n",
    "Finding an exact solution is not simple to do by hand, which means we'll need to use gradient descent.  \n",
    "We'll start by creating an error function to minimize.  \n",
    "For stochastic gradient descent, we'll just want the squared error corresponding to a single prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(x_i, y_i, beta):\n",
    "    return y_i - predict(x_i, beta)\n",
    "\n",
    "def squared_error(x_i, y_i, beta):\n",
    "    return error(x_i, y_i, beta) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know calculus, you can calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error_gradient(x_i, y_i, beta):\n",
    "    \"\"\" the gradient (with respect to beta) corresponding to the ith squared error term \"\"\"\n",
    "    return [-2 * x_ij * error(x_i, y_i, beta) for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're ready to find the optimal beta using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.625234786488353,\n",
       " 0.9715448288696535,\n",
       " -1.8679272872032218,\n",
       " 0.911456949921445]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_beta(x, y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(squared_error,\n",
    "                               squared_error_gradient,\n",
    "                               x, \n",
    "                               y, \n",
    "                               beta_initial,\n",
    "                               0.001)\n",
    "random.seed(0)\n",
    "beta = estimate_beta(x, daily_minutes_good)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results means that our model looks like:  \n",
    "\n",
    "minutes = 30.63 + 0.972friends - 1.868work hours + 0.911PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think of the coefficients of the model as representing all-else-being-equal estimate of the impacts of each factor.  \n",
    "All else being equal, each additional friend corresponds to an extra minute spent on the site each day.  \n",
    "All else being equal, each additional hour in a user's workday corresponds to about two fewer minutes spent on the site each day.  \n",
    "All else being equal, having a PhD is associated with spending an extra minute on the site each day.  \n",
    "What this *doesn't* tell us (directly, at least) is anything about the interactions among the variables.  \n",
    "It's possible that the effect of work hours is different for people with many friends than it is for people with few friends.  \n",
    "This model doen't capture that.  \n",
    "One way to handle this case is to introduce a new variable that is the *product* of \"friends\" and \"work hours\".  \n",
    "This effectively allows the \"work hours\" coefficient to increase or decrease as the number of friends increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible that the more friends you have, the more time you spend on the site *up to a point*, after which further friends cause you to spend less time on the site. (Perhaps with too many friends the experience is too overwhelming?)    \n",
    "We could try to capture this in our model by adding another variable that's the *square* of the number of friends.  \n",
    "Once we start adding variables, we need to worry about whether their coefficients actually \"matter\".  \n",
    "There are no limits to the numbers of products, logs, squares, and higher powers we could add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can look at the R-squared, which has now increased to 0.68:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiple_r_squared(x, y, beta):\n",
    "    sum_of_squared_errors = sum(error(x_i, y_i, beta) ** 2 for x_i, y_i in zip(x, y))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, however, that adding new variables to a regression will *necessarily* increase the R-squared.  \n",
    "After all, the simple regression model is just the special case of the multiple regression model where the coefficients on \"work hours\" and \"PhD\" both equal 0.  \n",
    "The optimal multiple regression model will necessarily have an error at least as small as that one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, in a multiple regression, we also need to look at the [standard errors](https://en.wikipedia.org/wiki/Standard_error) of the coefficients, which measure how certain we are about our estimates of each $\\beta_i$.  \n",
    "The regression as a whole may fit our data very well, but if some of the independent variables are correlated (or irrelevant), their coefficients might not *mean* much.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical approach to measuring these errors starts with another assumption -- that the errors $\\epsilon_i$ are independent normal random variables with mean 0 and some shared (unknown) standard deviation $\\sigma$.  \n",
    "In that case, we (or, more likely, our statistical software) can use some linear algebra to find the standard error of each coefficient.  \n",
    "The larger it is, the less sure our model is about that coefficient.  \n",
    "Unfortunately, we're not set up to that kind of linear algebra from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression: The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a sample of $n$ data points, generated by some (unknown to us) distribution:  \n",
    "\n",
    "`data = get_sample(num_points=n)`  \n",
    "\n",
    "In Chapter 5, we wrote a function to calculate the `median` of the observed data, which we can use as an estimate of the median of the distribution itself.  \n",
    "But how confident can we be about our estimate?  \n",
    "If all of the data in the sample are very close to 100, then it seems likely that the actual median is close to 100.  \n",
    "If approximately half of the data in the sample is close to 0 and the other half is close to 200, then we can't be nearly as certain about the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could repeatedly get new samples, we could compute the median of each and look at the distribution of those medians.  \n",
    "Usually we can't.  \n",
    "What we can do instead is [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) new data sets by choosing $n$ data points *with replacement* from our data and then calculate the medians of those synthetic data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    \"\"\" randomly samples len(data) elements with replacement \"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data, stats_fn, num_samples):\n",
    "    \"\"\" evaluates stats_fn on num_samples bootstrap samples from data \"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider the two following data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99.67232348413654,\n",
       " 99.66789337232244,\n",
       " 100.48755198519261,\n",
       " 100.05092001929478,\n",
       " 99.65667758671324,\n",
       " 100.23220072672623,\n",
       " 99.91858666365682,\n",
       " 100.0227656713815,\n",
       " 100.23660366112027,\n",
       " 100.1164465479239,\n",
       " 100.42194541502072,\n",
       " 99.74150836873142,\n",
       " 99.50983790912068,\n",
       " 99.74437835042211,\n",
       " 99.78741706803562,\n",
       " 99.59398218944757,\n",
       " 99.91917053096665,\n",
       " 100.18038503075336,\n",
       " 100.00163894294414,\n",
       " 99.79919455670316,\n",
       " 100.11076927840307,\n",
       " 99.64121932944245,\n",
       " 100.44445441120023,\n",
       " 99.67041626825599,\n",
       " 99.62757797471795,\n",
       " 99.79085513668522,\n",
       " 99.91871634648645,\n",
       " 100.07078532815683,\n",
       " 99.77176948360089,\n",
       " 99.83989472198373,\n",
       " 100.3596932989131,\n",
       " 100.2521018949548,\n",
       " 99.9011592314215,\n",
       " 100.12778552117875,\n",
       " 99.65014280209458,\n",
       " 100.32407018238091,\n",
       " 99.54672792219513,\n",
       " 99.5170433466829,\n",
       " 100.47463774250083,\n",
       " 100.19886179306465,\n",
       " 99.77952080966868,\n",
       " 100.23045831494373,\n",
       " 99.60358713212436,\n",
       " 99.57683422681276,\n",
       " 100.35035329379498,\n",
       " 100.48721391040483,\n",
       " 99.54927213964649,\n",
       " 99.88621092049782,\n",
       " 100.36301848326202,\n",
       " 100.2793428410688,\n",
       " 99.82619394533852,\n",
       " 99.7574493353493,\n",
       " 100.44537502266766,\n",
       " 100.04779962722975,\n",
       " 100.04093770521457,\n",
       " 100.08018406587479,\n",
       " 99.66100359041027,\n",
       " 99.7394486311777,\n",
       " 100.04219451967033,\n",
       " 99.67292226293546,\n",
       " 99.79447257565064,\n",
       " 100.16479042269945,\n",
       " 100.3897221304012,\n",
       " 99.72568693535783,\n",
       " 100.11936762284115,\n",
       " 99.75781287223258,\n",
       " 100.37094058751788,\n",
       " 100.12642764830203,\n",
       " 100.16134473286708,\n",
       " 99.52100403303243,\n",
       " 100.43986483241163,\n",
       " 99.51868493076591,\n",
       " 99.76388054551154,\n",
       " 99.94438370321161,\n",
       " 100.13431656883367,\n",
       " 100.34084229439202,\n",
       " 100.1690550404728,\n",
       " 100.25759293258176,\n",
       " 100.2676198834986,\n",
       " 100.27573851328239,\n",
       " 100.18941368574933,\n",
       " 99.87854525749272,\n",
       " 99.68706182891272,\n",
       " 99.5235309388243,\n",
       " 100.21022169010914,\n",
       " 100.28491643207317,\n",
       " 99.84888107500696,\n",
       " 100.42097103587746,\n",
       " 99.82313774814465,\n",
       " 100.2969232479075,\n",
       " 99.59066978027694,\n",
       " 100.35331089097097,\n",
       " 99.71488875825895,\n",
       " 100.4512853709461,\n",
       " 99.76783616641127,\n",
       " 100.40125854180143,\n",
       " 100.46839468595535,\n",
       " 100.02951482374817,\n",
       " 99.58558715062456,\n",
       " 100.36341870091441,\n",
       " 99.55814785576611]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 101 points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "close_to_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99.88123934680546,\n",
       " 0.07338681934354963,\n",
       " 0.06750332777081425,\n",
       " 0.09038014271638684,\n",
       " 0.13555032098946795,\n",
       " 0.15877205414328543,\n",
       " 0.09003766654085732,\n",
       " 0.43257503723419066,\n",
       " 0.7074066046516575,\n",
       " 0.49166968165200353,\n",
       " 0.2424559633954838,\n",
       " 0.4836258928392764,\n",
       " 0.531216554944571,\n",
       " 0.23869991541065028,\n",
       " 0.3175713118522211,\n",
       " 0.49244511541480107,\n",
       " 0.48312920949129423,\n",
       " 0.37848760329102427,\n",
       " 0.07212722630772073,\n",
       " 0.9510078771184474,\n",
       " 0.2696035016653002,\n",
       " 0.3098656501775928,\n",
       " 0.9379520613861619,\n",
       " 0.41128674425523026,\n",
       " 0.7340661885033657,\n",
       " 0.13960513771125882,\n",
       " 0.6719249710917461,\n",
       " 0.8133157262876995,\n",
       " 0.9610729439040515,\n",
       " 0.46064650262496654,\n",
       " 0.5994097716334423,\n",
       " 0.791383888512169,\n",
       " 0.7929383886152752,\n",
       " 0.35476637607517914,\n",
       " 0.6037083473955032,\n",
       " 0.9251244198229643,\n",
       " 0.38207224438305765,\n",
       " 0.6508039786753007,\n",
       " 0.337311253546157,\n",
       " 0.9889617627300915,\n",
       " 0.8479466705917126,\n",
       " 0.9380136613197754,\n",
       " 0.890544699972853,\n",
       " 0.6999668375199836,\n",
       " 0.5722156351304395,\n",
       " 0.31457994819943513,\n",
       " 0.3592733619762747,\n",
       " 0.4980503917157889,\n",
       " 0.24749071087507168,\n",
       " 0.4621494004356913,\n",
       " 0.4424887437917391,\n",
       " 200.73404813148971,\n",
       " 200.8032542676831,\n",
       " 200.03441456618614,\n",
       " 200.04042139830705,\n",
       " 200.0220118191685,\n",
       " 200.7943071206852,\n",
       " 200.34533862502275,\n",
       " 200.90231338846152,\n",
       " 200.95463155505186,\n",
       " 200.35290627275305,\n",
       " 200.80964881713547,\n",
       " 200.9304637922618,\n",
       " 200.394331375632,\n",
       " 200.14442737011086,\n",
       " 200.96257189194037,\n",
       " 200.42948706495028,\n",
       " 200.39685826980119,\n",
       " 200.86162297113142,\n",
       " 200.03541078390245,\n",
       " 200.85242302566837,\n",
       " 200.32769342519742,\n",
       " 200.08187070628674,\n",
       " 200.59699705816595,\n",
       " 200.62144953892806,\n",
       " 200.6927084494901,\n",
       " 200.55815050488226,\n",
       " 200.22981995135845,\n",
       " 200.3673810956228,\n",
       " 200.43652552161802,\n",
       " 200.6886802723357,\n",
       " 200.33302860827385,\n",
       " 200.69373016440153,\n",
       " 200.93652385018316,\n",
       " 200.9698129284526,\n",
       " 200.08178177744344,\n",
       " 200.74427048345936,\n",
       " 200.58524878064506,\n",
       " 200.59539519845202,\n",
       " 200.17488833775994,\n",
       " 200.6106991368175,\n",
       " 200.84816654570227,\n",
       " 200.99196298561083,\n",
       " 200.94596504182792,\n",
       " 200.15387287248996,\n",
       " 200.88708146275314,\n",
       " 200.2673913660276,\n",
       " 200.3597702511875,\n",
       " 200.88160086196595,\n",
       " 200.47914931241175,\n",
       " 200.03634068155228]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 101 points, 50 of them near 0, 50 of them near 200\n",
    "far_from_100 = ([99.5 + random.random()] + \n",
    "                [random.random() for _ in range(50)] + \n",
    "                [200 + random.random() for _ in range(50)])\n",
    "far_from_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.022765671\n",
      "99.8812393468\n"
     ]
    }
   ],
   "source": [
    "# If you calculate the median of each, both will be very close to 100.\n",
    "print median(close_to_100)\n",
    "print median(far_from_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100.00163894294414,\n",
       " 99.91917053096665,\n",
       " 100.04779962722975,\n",
       " 100.07078532815683,\n",
       " 100.05092001929478,\n",
       " 99.75781287223258,\n",
       " 100.02951482374817,\n",
       " 99.9011592314215,\n",
       " 100.05092001929478,\n",
       " 99.84888107500696,\n",
       " 100.16134473286708,\n",
       " 100.02951482374817,\n",
       " 99.94438370321161,\n",
       " 99.87854525749272,\n",
       " 100.04779962722975,\n",
       " 99.9011592314215,\n",
       " 99.91858666365682,\n",
       " 100.11076927840307,\n",
       " 100.08018406587479,\n",
       " 99.91917053096665,\n",
       " 99.84888107500696,\n",
       " 99.91917053096665,\n",
       " 100.00163894294414,\n",
       " 100.00163894294414,\n",
       " 100.00163894294414,\n",
       " 100.04219451967033,\n",
       " 100.04779962722975,\n",
       " 100.04093770521457,\n",
       " 100.02951482374817,\n",
       " 99.91871634648645,\n",
       " 100.11076927840307,\n",
       " 100.00163894294414,\n",
       " 100.04093770521457,\n",
       " 100.05092001929478,\n",
       " 100.00163894294414,\n",
       " 99.94438370321161,\n",
       " 99.94438370321161,\n",
       " 100.04219451967033,\n",
       " 100.04779962722975,\n",
       " 99.91871634648645,\n",
       " 100.05092001929478,\n",
       " 99.9011592314215,\n",
       " 100.02951482374817,\n",
       " 100.0227656713815,\n",
       " 100.16134473286708,\n",
       " 99.91871634648645,\n",
       " 100.00163894294414,\n",
       " 100.04219451967033,\n",
       " 100.02951482374817,\n",
       " 99.91871634648645,\n",
       " 100.08018406587479,\n",
       " 99.91871634648645,\n",
       " 100.04093770521457,\n",
       " 100.04219451967033,\n",
       " 99.94438370321161,\n",
       " 100.11076927840307,\n",
       " 99.91858666365682,\n",
       " 99.83989472198373,\n",
       " 100.04779962722975,\n",
       " 99.83989472198373,\n",
       " 100.00163894294414,\n",
       " 100.0227656713815,\n",
       " 99.82619394533852,\n",
       " 100.08018406587479,\n",
       " 100.02951482374817,\n",
       " 100.04779962722975,\n",
       " 100.00163894294414,\n",
       " 99.94438370321161,\n",
       " 99.91917053096665,\n",
       " 99.94438370321161,\n",
       " 100.0227656713815,\n",
       " 100.04219451967033,\n",
       " 100.02951482374817,\n",
       " 100.04219451967033,\n",
       " 99.9011592314215,\n",
       " 99.91858666365682,\n",
       " 99.91871634648645,\n",
       " 99.91871634648645,\n",
       " 99.91858666365682,\n",
       " 99.91871634648645,\n",
       " 99.91858666365682,\n",
       " 99.91871634648645,\n",
       " 100.04093770521457,\n",
       " 99.91917053096665,\n",
       " 100.0227656713815,\n",
       " 100.04219451967033,\n",
       " 100.00163894294414,\n",
       " 100.04093770521457,\n",
       " 100.11936762284115,\n",
       " 100.04093770521457,\n",
       " 99.94438370321161,\n",
       " 100.00163894294414,\n",
       " 100.00163894294414,\n",
       " 100.04093770521457,\n",
       " 100.00163894294414,\n",
       " 100.07078532815683,\n",
       " 100.05092001929478,\n",
       " 99.91917053096665,\n",
       " 100.04779962722975,\n",
       " 100.00163894294414]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, if you look at:\n",
    "bootstrap_statistic(close_to_100, median, 100)\n",
    "# you will mostly see numbers close to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200.03634068155228,\n",
       " 200.08178177744344,\n",
       " 0.8133157262876995,\n",
       " 0.9510078771184474,\n",
       " 99.88123934680546,\n",
       " 0.9510078771184474,\n",
       " 200.32769342519742,\n",
       " 200.0220118191685,\n",
       " 200.04042139830705,\n",
       " 0.9610729439040515,\n",
       " 0.9889617627300915,\n",
       " 200.0220118191685,\n",
       " 200.03541078390245,\n",
       " 0.9380136613197754,\n",
       " 200.0220118191685,\n",
       " 0.8479466705917126,\n",
       " 0.8133157262876995,\n",
       " 0.9379520613861619,\n",
       " 200.08178177744344,\n",
       " 0.9610729439040515,\n",
       " 200.0220118191685,\n",
       " 200.03634068155228,\n",
       " 0.9889617627300915,\n",
       " 99.88123934680546,\n",
       " 0.9510078771184474,\n",
       " 99.88123934680546,\n",
       " 200.14442737011086,\n",
       " 200.03634068155228,\n",
       " 200.0220118191685,\n",
       " 0.791383888512169,\n",
       " 200.0220118191685,\n",
       " 200.03634068155228,\n",
       " 200.08187070628674,\n",
       " 99.88123934680546,\n",
       " 0.9251244198229643,\n",
       " 0.9251244198229643,\n",
       " 200.08187070628674,\n",
       " 0.9380136613197754,\n",
       " 0.9379520613861619,\n",
       " 0.9889617627300915,\n",
       " 200.03634068155228,\n",
       " 200.03541078390245,\n",
       " 200.0220118191685,\n",
       " 99.88123934680546,\n",
       " 0.9510078771184474,\n",
       " 200.32769342519742,\n",
       " 200.03634068155228,\n",
       " 200.08178177744344,\n",
       " 0.9380136613197754,\n",
       " 200.03634068155228,\n",
       " 200.0220118191685,\n",
       " 0.9380136613197754,\n",
       " 0.9510078771184474,\n",
       " 99.88123934680546,\n",
       " 0.9889617627300915,\n",
       " 0.9610729439040515,\n",
       " 0.9889617627300915,\n",
       " 200.03634068155228,\n",
       " 0.8133157262876995,\n",
       " 0.9610729439040515,\n",
       " 200.03441456618614,\n",
       " 0.9889617627300915,\n",
       " 0.9379520613861619,\n",
       " 200.03441456618614,\n",
       " 0.9380136613197754,\n",
       " 0.9380136613197754,\n",
       " 200.08178177744344,\n",
       " 0.9610729439040515,\n",
       " 0.9610729439040515,\n",
       " 200.08178177744344,\n",
       " 0.9380136613197754,\n",
       " 200.03441456618614,\n",
       " 99.88123934680546,\n",
       " 200.0220118191685,\n",
       " 200.14442737011086,\n",
       " 200.35290627275305,\n",
       " 0.9379520613861619,\n",
       " 0.9510078771184474,\n",
       " 0.9251244198229643,\n",
       " 200.03441456618614,\n",
       " 99.88123934680546,\n",
       " 200.15387287248996,\n",
       " 0.9510078771184474,\n",
       " 200.14442737011086,\n",
       " 0.9889617627300915,\n",
       " 200.03634068155228,\n",
       " 0.9251244198229643,\n",
       " 0.890544699972853,\n",
       " 200.03541078390245,\n",
       " 0.9889617627300915,\n",
       " 0.890544699972853,\n",
       " 200.03441456618614,\n",
       " 0.7340661885033657,\n",
       " 0.9510078771184474,\n",
       " 0.9889617627300915,\n",
       " 200.03634068155228,\n",
       " 200.33302860827385,\n",
       " 200.0220118191685,\n",
       " 0.9379520613861619,\n",
       " 200.03541078390245]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now looking at:\n",
    "bootstrap_statistic(far_from_100, median, 100)\n",
    "# you will see a lot of numbers close to 0 and a lot of numbers close to 200 (with a few close to 100 thrown in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the `standard_deviation` of the first set of medians is close to 0, while the `standard_deviation` of the second set of medians is close to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.303183667387\n",
      "0.0681205300196\n",
      "100.021771234\n",
      "95.4570184486\n"
     ]
    }
   ],
   "source": [
    "print standard_deviation(close_to_100)\n",
    "print standard_deviation(bootstrap_statistic(close_to_100, median, 100))\n",
    "print standard_deviation(far_from_100)\n",
    "print standard_deviation(bootstrap_statistic(far_from_100, median, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extreme case would be easy to figure out by manually inspecting the data, but in real practice that won't usually be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Errors of Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can take the same approach to estimating the standard errors of our regression coefficients.  \n",
    "We repeatedly take a `bootstrap_sample` of our data and estimate `beta` based on that sample.  \n",
    "If the coefficient corresponding to one of the independent variables (say `num_friends`) doesn't vary much across samples, then we can be confident that our estimate is relatively accurate.  \n",
    "If the coefficient varies greatly across samples, then we can't be at all confident of our estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sampling, it is *very* important that we `zip` the `x` data and `y` data to make sure that the corresponding values of the independent and dependent variables are sampled together.  \n",
    "This means that `bootstrap_sample` will return a list of pairs (`x_i, y_i`), which must be reassembled into an `x_sample` and a `y_sample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30.126427794849263,\n",
       "  0.9012617032007041,\n",
       "  -1.7030126679618276,\n",
       "  0.33505058469292937],\n",
       " [30.739799574784413,\n",
       "  0.9315551053594878,\n",
       "  -1.8441042227777509,\n",
       "  0.8586836390433148],\n",
       " [29.412830031142178,\n",
       "  1.0698124634313169,\n",
       "  -1.7510563652088023,\n",
       "  1.151878332200066],\n",
       " [30.562978707724948,\n",
       "  1.2020018751810237,\n",
       "  -2.008914519534357,\n",
       "  0.9248718484382588],\n",
       " [31.87041480154315,\n",
       "  0.9571614075937392,\n",
       "  -1.9366453588430825,\n",
       "  -0.12061673914648428]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_sample_beta(sample):\n",
    "    \"\"\" sample is a list of pairs (x_i, y_i) \"\"\"\n",
    "    x_sample, y_sample = zip(*sample)\n",
    "    return estimate_beta(x_sample, y_sample)\n",
    "\n",
    "random.seed(0)\n",
    "bootstrap_betas = bootstrap_statistic(zip(x, daily_minutes_good), estimate_sample_beta, 100)\n",
    "bootstrap_betas[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can estimate the standard deviation of each coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.174097542924062,\n",
       " 0.07861006463889537,\n",
       " 0.13138388603694567,\n",
       " 0.9899022849002838]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_standard_errors = [standard_deviation([beta[i] for beta in bootstrap_betas]) for i in range(4)]\n",
    "bootstrap_standard_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [1.174,    # constant term,  actual error = 1.19\n",
    "#  0.079,    # num_friends,    actual error = 0.080\n",
    "#  0.131,    # unemployed,     actual error = 0.127\n",
    "#  0.990]    # phd,            actual error = 0.998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these results to test hypotheses such as \"does $\\beta_i$ equal zero?\"  \n",
    "Under the null hypothesis $\\beta_i = 0$ (and with our other assumptions about the distribution of $\\epsilon_i$) the statistic:  \n",
    "\n",
    "$ \\Large t_j = \\hat\\beta_j \\;/\\; \\hat\\sigma_j$  \n",
    "\n",
    "which is our estimate of $\\beta_j$ divided by our estimate of its standard error, follows a [Student's t-distribution](https://en.wikipedia.org/wiki/Student's_t-distribution) with \"$n - k$ [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a `students_t_cdf()` function, we could calculate *p*-values for each least_squares coefficient to indicate how likely we would be to observe such a value if the actual coefficient were zero.  \n",
    "Unfortunately, we don't have such a function (although we would if we weren't working from scratch).  \n",
    "However, as degrees of freedom get large, the *t*-distribution gets closer to a standard normal.  \n",
    "In a situation like this, where *n* is much larger than *k*, we can use `normal_cdf()` and still feel good about ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.357467198817\n"
     ]
    }
   ],
   "source": [
    "def p_value(beta_hat_j, sigma_hat_j):\n",
    "    if beta_hat_j > 0:\n",
    "        # if the coefficient is positive, calculate twice the probability of seeing an even larger value\n",
    "        return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        # otherwise twice the probability of seeing a smaller value\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "    \n",
    "print(p_value(30.63, 1.174))   # ~0      constant term\n",
    "print(p_value(0.972, 0.079))   # ~0      num_friends\n",
    "print(p_value(-1.868, 0.131))  # ~0      work_hours\n",
    "print(p_value(0.911, 0.990))   #  0.36   PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a situation not like this, we would probably be using statistical software that knows how to calculate the *t*-distribution, as well as how to calculate the exact standard errors.  \n",
    "While most of the coefficients have very small *p*-values (suggesting that they are indeed nonzero), the coefficient for \"PhD\" is not 'significantly' different from zero, which makes it likely that the coefficient for \"PhD\" is random rather than meaningful.  \n",
    "In more elaborate regression scenarios, you sometimes want to test more elaborate hypotheses about the data, such as:  \n",
    "\"at least one of the $\\beta_j$ is non-zero\", or  \n",
    "\"$\\beta_1$ equals $\\beta_2$ *and* $\\beta_3$ equals $\\beta_4$\",  \n",
    "which can be done with an [F-test](https://en.wikipedia.org/wiki/F-test), which is, unfortunately, outside the scope of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
