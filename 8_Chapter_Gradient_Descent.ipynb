{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have some function f that takes a sinput a vector of real numbers and outputs a single real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\" computes the sum of squared elements in v \"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will frequently need to maximize or minimize such functions.  \n",
    "That is, we need to find the input v that produces the largest or smallest possible value.  \n",
    "For functions like ours, the [gradient](https://en.wikipedia.org/wiki/Gradient)(in calculus, this is the vector of partial derivatives) gives the input direction in which the function most quickly increases.  \n",
    "Accordingly, one approach to maximizing a function is to pick a random starting point, compute the gradient, take a small step in the direction of the gradient (the direction that causes the function to increase the most), and repeat with a new starting point.  \n",
    "Similarly, you can try to minimize a function by taking small steps in the *opposite* direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "If a function has a unique global minimum, this procedure is likely to find it.  \n",
    "If a function has multiple local minima, gradient descent might find the wrong one of them, in which case you might re-run the procedure from a variety of starting points.  \n",
    "If a function has no minimum, then i't possible the procedure might go on forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
