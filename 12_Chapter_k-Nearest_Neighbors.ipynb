{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12. k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from linear_algebra import distance\n",
    "from statistics import mean\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you're trying to predict how I'm going to vote in the next presidential election.  \n",
    "If you know nothing else about me, one sensible approach is to look at how my *neighbors* are planning to vote.  \n",
    "Now imagine that you know more about me than just geography -- my age, my income, marital status, and so on.  \n",
    "To the extent that my behavior is influenced (or characterized) by those things, looking just at my neighbors who are close to me among all of those dimensions seems likely to be an even better predictor than looking at all of my neighbors.  \n",
    "This is the idea behind [nearest neighbors classification](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nearest neighbors requires:\n",
    "- Some notion of distance.\n",
    "- An assumption that points that are close to one another are similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN deliberately neglects a lot of information, since the prediction for each new point depends only on the handful of points closest to it.  \n",
    "What's more, KNN is probably not going to help you understand the drivers of whatever phenomenon you're looking at.  \n",
    "In the general situation, we have some data points and a corresponding set of labels that can be binary (eg.T/F), categorical, or numeric.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, the data points will be vectors, so we can use the `distance` function from Chapter 4.  \n",
    "Let's say we've picked a number `k` like 3 or 5.  \n",
    "When we want to classify some new data point, we find the `k` nearest labeled points and let them vote on the new output.  \n",
    "To do this, we need a function that counts votes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_majority_vote(labels):\n",
    "    votes = Counter(labels)\n",
    "    winner, _ = votes.most_common(1)[0]\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good start, but what about tie votes?  \n",
    "We have several options:\n",
    "- Pick one of the winners at random.\n",
    "- Weight the votes by distance and pick the weighted winner.\n",
    "- Reduce `k` until we find a unique winner.  \n",
    "\n",
    "Let's implement the third option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majority_vote(labels):\n",
    "    \"\"\" assumes that labels are ordered from nearest to farthest \"\"\"\n",
    "    vote_counts = Counter(labels)\n",
    "    winner, winner_count = vote_counts.most_common(1)[0]\n",
    "    num_winners = len([count for count in vote_counts.values() if count == winner_count])\n",
    "    if num_winners == 1:\n",
    "        return winner   # unique winner, so return it\n",
    "    else:\n",
    "        return majority_vote(labels[:-1])   # try again without the furthest data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach will work eventually, since in the worst case we go all the way down to just one label, at which point that one label wins.  \n",
    "With this function, it's easy to create a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_clasify(k, labeled_points, new_point):\n",
    "    \"\"\" each labeled point should be a pair (point, label) \"\"\"\n",
    "    # order the labeled points from nearest to farthest\n",
    "    by_distance = sorted(labeled_points,\n",
    "                         key=lambda (point, _):\n",
    "                         distance(point, new_point))\n",
    "    # find the labels for the closest k\n",
    "    k_nearest_labels = [label for _, label in by_distance[:k]]\n",
    "    # and let them vote\n",
    "    return majority_vote(k_nearest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Favorite Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
