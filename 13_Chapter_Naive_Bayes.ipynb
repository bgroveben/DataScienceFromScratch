{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from machine_learning import split_data\n",
    "import math, random, re, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSciencester has a popular feature that allows members to send messages to other members.  \n",
    "The VP of DataSciencester has tasked you with building a spam filter for those messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Really Dumb Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember Bayes' Theorem?  \n",
    "\n",
    "${\\large\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}}$  \n",
    "\n",
    "where A and B are events and P(B) â‰  0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine a 'universe' that consists of receiving a message chosen randomly from all possible messages.  \n",
    "Let **`S`** be the event \"the message is spam\" and **`V`** be the event \"the message contains the word *viagra*.\"  \n",
    "Bayes' Theorem tells us that the probability that the message is spam *conditional* on containing the word viagra is:  \n",
    "\n",
    "${\\large\\displaystyle P(S\\mid V)={\\frac {P(V\\mid S)\\,P(S)}{P(V\\mid S)\\,P(S) + P(V\\mid \\neg S)P(\\neg S)}}}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerator is the probability that a message is spam *and* contains 'viagra', while the denominator is the probability that a message contains 'viagra'.  \n",
    "Think of this calculation as representing the proportion of 'viagra' messages that are spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a large corpus of messages that we know are spam, and a large collection of messages that we know are *not* spam, then we can estimate ${P(V\\mid S)}$ and ${P(V\\mid \\neg S)}$.  \n",
    "If we further assume that any message is equally likely to be spam or not-spam ( ${P(S) = 0.5}$ and ${P(\\neg S) = 0.5}$ ), then:  \n",
    "\n",
    "${\\large\\displaystyle P(S\\mid V)={\\frac {P(V\\mid S)\\,}{P(V\\mid S) + P(V\\mid \\neg S)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if 50% of spam messages have the word *viagra*, but only 1% of nonspam messages do, then the probability that any given *viagra*-containing email is spam is:  \n",
    "\n",
    "${\\large\\displaystyle {\\frac {0.5}{0.5 \\,+\\, 0.01} = {98\\%}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Sophisticated Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a vocabulary, or corpus, of many words $w_1, w_2, ..., w_n$.  \n",
    "To move this into the realm of probability theory, we'll write $X_i$ for the event \"a message that contains the word $w_i$.\"  \n",
    "Also imagine that we've come up with an estimate ${P(X_i \\mid S)}$ for the probability that a spam message contains the *i*th word, and a similar estimate ${P(X_i \\mid \\neg S)}$ for the probability that a non-spam message contains the *i*th word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to Naive Bayes is making the assumption that the presence or absence of each word are independent of one another, conditional on a message being spam or not.  \n",
    "Intuitively, this assumption means that knowing whether a certain spam message contains the word *viagra* gives you no information about whether or not that same message contains the word *rolex*.  \n",
    "In math terms, this means that:  \n",
    "${P(X_1 = x_1, X_1 = x_2, ..., X_n = x_n \\mid S) = P(X_1 = x_1 \\mid S)\\;\\times\\;...\\;\\times\\;P(X_n = x_n \\mid S)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extreme assumption.  \n",
    "Imagine that our vocabulary consists *only* of the words 'viagra' and 'rolex', and that half of all spam messages are for 'cheap viagra' and that the other half are for 'authentic rolex'.  \n",
    "In this case, the Naive Bayes estimate that a spam message contains both *viagra* and *rolex* is:  \n",
    "\n",
    "${P(X_1 = 1, X_2 = 1 \\mid S) = P(X_1 = 1 \\mid S)P(X_2 = 1 \\mid S) = .5 \\times .5 = .25}$  \n",
    "\n",
    "since we've assumed away the knowledge that *viagra* and *rolex* actually never occur together.  \n",
    "Although this assumption may seem unrealistic and unreasonable, this model often performs well and isused in actual spam filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same Bayes' Theorem reasoning we used for our 'viagra-only' spam filter tells us that we can calculate the probability that a message is spam using the equation:  \n",
    "\n",
    "${\\normalsize\\displaystyle {P(S \\mid X = x)} = {\\frac {P(X = x \\mid S)}{P(X = x \\mid S) + P(X = x \\mid \\neg S)}}}$  \n",
    "\n",
    "The Naive Bayes assumption allows us to calculate each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you usually want to avoid multiplying lots of probabilities together, to avoid a problem called [underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow).  \n",
    "Basically, underflow is a result of computers not dealing well floating-point numbers that are too close to zero.  \n",
    "Recalling from algebra that ${log\\;(ab)\\; = log\\;a + log\\;b}$ and that ${exp\\;(log\\;x) = x}$, we usually calculate ${p_1 \\times \\;p_2\\;\\times\\;...\\;\\times p_n}$ as the equivalent:  \n",
    "\n",
    "${\\large\\displaystyle {exp\\;( log(p_1)\\;+\\;...\\;+\\;log(p_n))}}$  \n",
    "\n",
    "The only challenge left is coming up with estimates for ${P(X_i \\mid S)}$ and ${P(X_i \\mid \\neg S)}$, which are the probabilities that a spam message (or nonspam message) contains the word ${w_i}$.  \n",
    "If we have a fair number of 'training' messages labeled as spam and not-spam, an obvious first try is to estimate ${P(X_i \\mid S)}$ simply as a fraction of spam messages containing word ${w_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This causes a big problem, though.  \n",
    "Imagine that in our training set the word 'data' only occurs in nonspam messages.  \n",
    "In that case, we would estimate ${P(\"data\" \\mid S) = 0}$.  \n",
    "The result is that our Naive Bayes classifier would always assign spam probability 0 to *any* message containing the word 'data', even a message like \"data on cheap viagra and authentic rolex watches.\"  \n",
    "To avoid this problem, we usually use some kind of [smoothing](https://en.wikipedia.org/wiki/Additive_smoothing).  \n",
    "In particular, we'll choose a [pseudocount](https://en.wikipedia.org/wiki/Pseudocount) -- *k* -- and estimate the probability of seeing the *i*th word in a spam message as:  \n",
    "\n",
    "${\\large P(X_i \\mid S) = {\\frac {(k\\;+\\; \\text{number of spam messages containing ${w_i}$)}}{ 2k\\;+\\;\\text{number of spam messages}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for ${P(X_i \\mid \\neg S)}$.  \n",
    "When calculating the spam probabilities for the *i*th word, we assume that we also saw *k* additional spams containing the word and *k* additional spams *not* containing the word.  \n",
    "For example, if 'data' occurs in 0/98 spam emails, and if *k* is 1, we can estimate:  \n",
    "${P(\"data\" \\mid S)}$ as 1/100 = 0.01,  \n",
    "which allows our classifier to still assign some nonzero spam probability to messages that contain the word 'data'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in this section is quite a bit to take in, so read it again before moving on to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's build this thing.  \n",
    "First, we'll create a function to tokenize messages into distinct words by:\n",
    "- converting each message to lowercase,\n",
    "- using `re.findall()` to extract the 'words' consisting of letters, numbers, and apostrophes,\n",
    "- using `set()` to get just the distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    # convert to lowercase\n",
    "    message = message.lower()\n",
    "    # extract the words\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)\n",
    "    # remove duplicates\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second function will count the words in a labeled training set of messages.  \n",
    "We'll have it return a dictionary whose keys are words, and whose values are two-element lists `[spam_count, non_spam_count]` corresponding to how many times we saw that word in both spam and non-spam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\" training set consists of pairs (message, is_spam) \"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next step is to turn these counts into estimated probabilities using the smoothing described above.  \n",
    "The function will return a list of triplets containing\n",
    "- each word, \n",
    "- the probability of seeing that word in a spam message, \n",
    "- and the probability of seeing that word in a non-spam message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\" turn the word_counts into a list of triplets w, p(w|spam), and p(w|not_spam) \"\"\"\n",
    "    return [(w,\n",
    "            (spam + k) / (total_spams + 2 * k),\n",
    "            (non_spam + k) / (total_non_spams + 2 * k))\n",
    "            for w, (spam, non_spam) in counts.iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece is to use these word probabilities (and our Naive Bayes assumptions) to assign probabilities to messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    # iterate through each word in the corpus/vocabulary\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        # if *word* appears in the message, add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "        # if *word* doesn't appear in the message, add the log probability of *not*\n",
    "        # seeing it, which is log(1 - probability of seeing it)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "        \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can put all of this together into our Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam for message, is_spam in training_set if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        # run the training data\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, num_spams, num_non_spams, self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model, we'll be using the [SpamAssasin public corpus](https://spamassassin.apache.org/publiccorpus/) (an oldie but a goodie).  \n",
    "If you want to play along, download the files prefixed with `20021010` and unzip them.  \n",
    "There should be three folders: `spam`, `easy_ham`, and `hard_ham`.  \n",
    "Each folder contains many emails, each contained in a single file.  \n",
    "In order to keep things *really* simple, we are only going to look at the subject lines of each email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do we identify the subject line?  \n",
    "Looking through the files, they all seem to start with \"Subject\", so let's look for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Re: New Sequences Window', False),\n",
       " ('[zzzzteana] RE: Alexander', False),\n",
       " ('[zzzzteana] Moscow bomber', False),\n",
       " (\"[IRR] Klez: The Virus That  Won't Die\", False),\n",
       " ('Re: Insert signature', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('[zzzzteana] Playboy wants to go out with a bang', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('[zzzzteana] Meaningful sentences', False)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, re\n",
    "\n",
    "# modify the path to wherever you put the files\n",
    "path = r\"spam_email_data/*/*\"\n",
    "data = []\n",
    "# glob.glob returns every filename that matches the wildcarded path\n",
    "for fn in glob.glob(path):\n",
    "    is_spam = \"ham\" not in fn\n",
    "    with open(fn,'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # remove the leading \"Subject: \" and keep what's left\n",
    "                subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                data.append((subject, is_spam))\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the data into training data and test data, and then we're ready to build a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_data, test_data = split_data(data, 0.75)\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check how the model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(False, False): 704,\n",
       "         (False, True): 33,\n",
       "         (True, False): 38,\n",
       "         (True, True): 101})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triplets (subject, actual is_spam, predicted spam probability)\n",
    "classified = [(subject, is_spam, classifier.classify(subject)) for subject, is_spam in test_data]\n",
    "# assume that spam_probability > 0.5 corresponds to spam prediction and\n",
    "# count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = Counter((is_spam, spam_probability > 0.5) for _, is_spam, spam_probability in classified)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A review of the results:\n",
    "- 704 True Negatives (ham classified as 'ham')\n",
    "- 33 False Positives (ham classified as 'spam')\n",
    "- 38 False Negatives (spam classified as 'ham')\n",
    "- 101 True Positives (spam classified as 'spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is  101 / (101 + 33) = 75%  \n",
    "Recall is  101 / (101 + 38) = 73%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the most misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spammiest hams: [('Attn programmers: support offered [FLOSS-Sarai Initiative]', False, 0.975612960514201), ('2000+ year old Greek computer reinterpreted', False, 0.983535500810437), ('What to look for in your next smart phone (Tech Update)', False, 0.9898719206903349), ('[ILUG-Social] Re: Important - reenactor insurance needed', False, 0.9995349057803377), ('[ILUG-Social] Re: Important - reenactor insurance needed', False, 0.9995349057803377)]\n",
      "\n",
      "hammiest_spams: [('Re: girls', True, 0.0009525186158414711), ('Introducing Chase Platinum for Students with a 0% Introductory APR', True, 0.0012566691211091526), ('.Message report from your contact page....//ytu855 rkq', True, 0.0015109358288617285), ('Testing a system, please delete', True, 0.0026920538836874555), ('Never pay for the goodz again (8SimUgQ)', True, 0.00591162322193142)]\n"
     ]
    }
   ],
   "source": [
    "# sort by spam_probability from smallest to largest\n",
    "classified.sort(key=lambda row: row[2])\n",
    "# the highest predicted spam probabilities among the non_spams\n",
    "spammiest_hams = filter(lambda row: not row[1], classified)[-5:]\n",
    "print \"spammiest hams: \" + str(spammiest_hams)\n",
    "print\n",
    "# the lowest predicted spam probabilities among the actual spams\n",
    "hammiest_spams = filter(lambda row: row[1], classified)[:5]\n",
    "print \"hammiest_spams: \" + str(hammiest_spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two spammiest hams both have the words 'needed' (77 times more likely to appear in spam),  \n",
    "'insurance' (30 times more likely to appear in spam),  \n",
    "and 'important' (10 times more likely to appear in spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hammiest spam is too short ('Re:girls') to make much of a judgment, and the second is a credit card offer with many words not included in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the spammiest *words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spammiest words are: [('year', 0.028767123287671233, 0.00022893772893772894), ('sale', 0.031506849315068496, 0.00022893772893772894), ('rates', 0.031506849315068496, 0.00022893772893772894), ('systemworks', 0.036986301369863014, 0.00022893772893772894), ('money', 0.03972602739726028, 0.00022893772893772894)]\n",
      "\n",
      "The hammiest words are: [('spambayes', 0.0013698630136986301, 0.04601648351648352), ('users', 0.0013698630136986301, 0.036401098901098904), ('razor', 0.0013698630136986301, 0.030906593406593408), ('zzzzteana', 0.0013698630136986301, 0.029075091575091576), ('sadev', 0.0013698630136986301, 0.026785714285714284)]\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\" use Bayes' Theorem to calculate p(spam | message contains word) \"\"\"\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "spammiest_words = words[-5:]\n",
    "print \"The spammiest words are: \" + str(spammiest_words)\n",
    "print\n",
    "hammiest_words = words[:5]\n",
    "print \"The hammiest words are: \" + str(hammiest_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to improve model performance  \n",
    "- More data. Nuff said.\n",
    "- Look at the message content, not just the subject line. Be careful how you deal with the message headers.\n",
    "- Our classifier takes into account every word that appears in the training set, even words that appear only once. Modify the classifier to accept an optional `min_count` threshold and ignore tokens that don't appear at least that many times.\n",
    "- The tokenizer has no notion of similar words (e.g. 'cheap' and 'cheapest'). Modify the classifier to take an optional `stemmer` function that converts words to [equivalence classes](https://en.wikipedia.org/wiki/Equivalence_class) of words, like the [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/). \n",
    "- Although our features are all of the form \"message contains word $w_i$\", there's no reason why this has to be the case. In our implementation, we could add extra features like \"message contains a number\" by creating phony tokens like *contains:number* and modifying the `tokenizer` to emit them when appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paul Graham's articles [A Plan for Spam](https://en.wikipedia.org/wiki/Equivalence_class) and [Better Bayesian Filtering](http://www.paulgraham.com/better.html) offer insight into the ideas behind building spam filters.  \n",
    "- scikit-learn contains a [BernoulliNB](http://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes) model that implements a similar Naive Bayes algorithm that was implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
