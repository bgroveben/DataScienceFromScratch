{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random\n",
    "\n",
    "def normal_cdf(x, mu=0,sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    \"\"\"find approximate inverse using binary search\"\"\"\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    low_z, low_p = -10.0, 0            # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z,  hi_p  =  10.0, 1            # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the cdf's value there\n",
    "        if mid_p < p:\n",
    "            # midpoint is still too low, search above it\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # midpoint is still too high, search below it\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will we do with all this statistics and probability theory?  \n",
    "The science part of data science frequently involves forming and testing hypotheses about our data and the processes that generate it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, [hypotheses](https://en.wikipedia.org/wiki/Hypothesis) are assertions like \"this coin is fair\" that can be translated into statistics about data.  \n",
    "In the classical setup, we have a null hypothesis $H_0$ that represents some default position, and an alternative hypothesis $H_1$ that we would like to compare it with.  \n",
    "We then use statistics to decide whether we can reject $H_0$ as false or not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Flipping a Coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a coin and we want to test whether it's fair or not.  \n",
    "We make the assumption that the coin has some probability $p$ of landing heads up, so our null hypothesis is that the coin is fair, or $p=0.5$.  \n",
    "The null hypothesis is tested against the alternative hypothesis $p\\neq0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test will involve flipping the coin some number $n$ times and counting the number of heads $X$.  \n",
    "Each coin flip is a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial), which means that X is a Binomial(n,p) random variable that can be approximated using the normal distribution that was covered in Chapter 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n, p):\n",
    "    \"\"\" finds mu and sigma corresponding to a Binomial(n, p) \"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p *(1 - p) * n)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a random variable follows a normal distribution, we can use <code>normal_cdf</code> to figure out the probability that its realized value lies within (or outside) a particular interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The normal_cdf is the probability that the variable is below a threshold\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# therefore, if it's not below the threshold, it's above the threshold\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# less than hi and not less than lo gives us between\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# and if it's not between, it's outside\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse -- find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood.  \n",
    "For example, if we want to find an interval centerd at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\" returns the z for which P(Z <= z) = probability \"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\" returns the z for which P(Z >= z) = probability \"\"\"\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\" returns the symmetric (about the mean) bounds that contain the specified probability \"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    # upper bound should have tail_probability above it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    # lower bound should have tail_probability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, let's say that we choose to flip the coin $n=1000$ times.  \n",
    "If our hypothesis of fairness is true, $X$ should be distributed approximately normally with mean 50 and standard deviation 15.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a decision about significance --  \n",
    "How willing are we to make a $type \\;1\\ error$ (false positive), in which we reject $H_0$ (the null hypothesis) even though it's true?\n",
    "For reasons lost to the annals of history (meaning it's common practice but no one can agree why), this willingness is often set at 5% or 1%.  \n",
    "We're going with 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the test that rejects $H_0$ if $X$ falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming $p$ really equals 0.5 (meaning $H_0$ is true), there is just a 5% chance that we observe an x that lis outside this interval, which is the exact significance that we wanted.  \n",
    "Said differently, if $H_0$ is true, then, approximately 19 times out of 20, this test will give the correct result.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also often interested in the $power$ of a test, which is the probability of not making a $type \\;2\\ error$ (false negative), in which we fail to reject $H_0$ even though it is false.  \n",
    "In order to measure this, we have to specify what exactly $H_0$ being false $means$.  \n",
    "In other words, knowing merely that $p$ is not 0.5 doesn't tell you very much about the distribution of X.  \n",
    "In particular, let's check what happens if $p$ is really 0.55, so that the coin is slightly biased towards heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, we can calculate the power of the test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.113451998705\n",
      "0.886548001295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8865480012953671"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 95% bounds based on the assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "\n",
    "# actual mu and sigma based on p = 0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "\n",
    "# a type 2 error means we fail to reject the null hypothesis, \n",
    "# which will happen when X is still in our original interval\n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(type_2_probability)\n",
    "print(power)\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine instead that our null hypothesis was that the coin is not biased towards heads, or that $p\\le0.5$.  \n",
    "In that case we want a <em>one-sided test</em> that rejects the null hypothesis when X is much larger than 50 but not when X is smaller than 50.  \n",
    "So, let's do a 5%-significance test using <code>normal_probability_below</code> to find the cutoff below which 95% of the probability lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526.0073585242053"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "hi  # is 526 (< 531, since we need more probability in the upper tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363794803307173"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more powerful test, since it no longer rejects $H_0$ when X is below 469 (which is unlikely to happen if $H_1$ is true)  \n",
    "and instead rejects $H_0$ when X is between 526 and 531 (which is somewhat likely to happen if $H_1$ is true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An alternative way of thinking about the preceding test involves p-values.  \n",
    "Instead of choosing bounds based on some probability cutoff, we compute the probability -- assuming $H_0$ is true -- that we would see a value at least as extreme as the one we actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our two-sided test of whether the coin is fair, we compute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598857"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        # if x is greater than the mean, the tail is what's greater than x\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        # if x is less than the mean, the tail is what's less than x\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "\n",
    "# If we were to see 530 heads, we would compute:\n",
    "two_sided_p_value(529.5, mu_0, sigma_0)  # mu_0 is 500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we choose 529.5 instead of 530?  \n",
    "This is called a [continuity correction](https://en.wikipedia.org/wiki/Continuity_correction).  \n",
    "It reflects the fact that <code>normal_probability_between(529.5, 530.5, mu_0, sigma_0)</code> is a better estimate of the probability of seeing 530 heads than <code>normal_probability_between(530, 531, mu_0, sigma_0)</code> is.  \n",
    "Correspondingly, <code>normal_probability_above(529.5, mu_0, sigma_0)</code> is a better estimate of the probability of seeing at least 530 heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to convince yourself that this is a sensible estimate is with a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06168\n"
     ]
    }
   ],
   "source": [
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    # count number of heads in 1000 flips\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 for _ in range(1000))\n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        # and count how often the number is 'extreme'\n",
    "        extreme_value_count += 1\n",
    "        \n",
    "print extreme_value_count / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is greater than our 5% significance, we <em>would not reject</em> the null hypothesis.  \n",
    "If instead we saw 532 heads, the p-value would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is smaller than the 5% significance, which means we <em> would reject</em> the null hypothesis.  \n",
    "It's the exact same test as before, just a different way of approaching the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our one-sided test, if we saw 525 heads we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582083"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(524.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means we <em>would not reject</em> the null hypothesis.  \n",
    "If we saw 527 heads, the computation would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we <em>would reject</em> the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caution\n",
    "Make sure your data is roughly normally distributed before using <code>normal_probability_above()</code> to compute p-values.  \n",
    "The annals of bad data science are filled with examples of people opining that the chance of some observed event occurring at random is one in a million, when what they really mean is \"the chance, assuming the data is distributed normally\", which is pretty meaningless if the data is not distributed normally.  \n",
    "There are various statistical tests for normality, but even plotting the data is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have been testing hypotheses about the value of the heads probability $p$, which is a $parameter$ of the unknown 'heads' distribution.   \n",
    "When this is the case, a third approach is to construct a <em>confidence interval</em> around the observed value of the parameter.  \n",
    "For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip -- 1 if heads, 0 if tails.  \n",
    "If we observe 525 heads out of 1000 flips, then we estimate $p$ equals 0.525.  \n",
    "How confident can we be about this estimate?  \n",
    "Well, if we knew the exact value of $p$, the Central Limit Theorem tells us that the average of those Bernoulli variables should be approximately normal, with mean $p$ and standard deviation <code>math.sqrt(p * (1 - p) / 1000)</code>.  \n",
    "Here, we don't know $p$, so instead we use our estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015791611697353755"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not entirely justified, but people seem to do it anyway. (???)  \n",
    "Using the normal approximation, we conclude that we are \"95 percent confident\" that the following interval contains the parameter $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "This is a statement about the $interval$, not about $p$.  \n",
    "You should understand it as the assertion that if you were to repeat the experiment many times, 95% of the time the \"true\" parameter (which is the same every time) would lie within the observed confidence interval (which might be different every time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we do not conclude that the coin is unfair, since 0.5 falls within our confidence interval.  \n",
    "If instead we had seen 540 heads, then we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma is 0.0157607106439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat \n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print \"Sigma is {}\".format(sigma)\n",
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"fair coin\", doesn't lie in the confidence interval.  \n",
    "In other words, the \"fair coin\" hypothesis doesn't pass a test that you would expect it to pass 95% of the time if it were true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
