{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normal_cdf(x, mu=0,sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    \"\"\"find approximate inverse using binary search\"\"\"\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    low_z, low_p = -10.0, 0            # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z,  hi_p  =  10.0, 1            # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the cdf's value there\n",
    "        if mid_p < p:\n",
    "            # midpoint is still too low, search above it\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # midpoint is still too high, search below it\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will we do with all this statistics and probability theory?  \n",
    "The science part of data science frequently involves forming and testing hypotheses about our data and the processes that generate it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, [hypotheses](https://en.wikipedia.org/wiki/Hypothesis) are assertions like \"this coin is fair\" that can be translated into statistics about data.  \n",
    "In the classical setup, we have a null hypothesis $H_0$ that represents some default position, and an alternative hypothesis $H_1$ that we would like to compare it with.  \n",
    "We then use statistics to decide whether we can reject $H_0$ as false or not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Flipping a Coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a coin and we want to test whether it's fair or not.  \n",
    "We make the assumption that the coin has some probability $p$ of landing heads up, so our null hypothesis is that the coin is fair, or $p=0.5$.  \n",
    "The null hypothesis is tested against the alternative hypothesis $p\\neq0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test will involve flipping the coin some number $n$ times and counting the number of heads $X$.  \n",
    "Each coin flip is a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial), which means that X is a Binomial(n,p) random variable that can be approximated using the normal distribution that was covered in Chapter 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n, p):\n",
    "    \"\"\" finds mu and sigma corresponding to a Binomial(n, p) \"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p *(1 - p) * n)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a random variable follows a normal distribution, we can use <code>normal_cdf</code> to figure out the probability that its realized value lies within (or outside) a particular interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The normal_cdf is the probability that the variable is below a threshold\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# therefore, if it's not below the threshold, it's above the threshold\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# less than hi and not less than lo gives us between\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# and if it's not between, it's outside\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse -- find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood.  \n",
    "For example, if we want to find an interval centerd at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\" returns the z for which P(Z <= z) = probability \"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\" returns the z for which P(Z >= z) = probability \"\"\"\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\" returns the symmetric (about the mean) bounds that contain the specified probability \"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    # upper bound should have tail_probability above it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    # lower bound should have tail_probability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, let's say that we choose to flip the coin $n=1000$ times.  \n",
    "If our hypothesis of fairness is true, $X$ should be distributed approximately normally with mean 50 and standard deviation 15.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a decision about significance --  \n",
    "How willing are we to make a $type \\;1\\ error$ (false positive), in which we reject $H_0$ (the null hypothesis) even though it's true?\n",
    "For reasons lost to the annals of history (meaning it's common practice but no one can agree why), this willingness is often set at 5% or 1%.  \n",
    "We're going with 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the test that rejects $H_0$ if $X$ falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming $p$ really equals 0.5 (meaning $H_0$ is true), there is just a 5% chance that we observe an x that lis outside this interval, which is the exact significance that we wanted.  \n",
    "Said differently, if $H_0$ is true, then, approximately 19 times out of 20, this test will give the correct result.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also often interested in the $power$ of a test, which is the probability of not making a $type \\;2\\ error$ (false negative), in which we fail to reject $H_0$ even though it is false.  \n",
    "In order to measure this, we have to specify what exactly $H_0$ being false $means$.  \n",
    "In other words, knowing merely that $p$ is not 0.5 doesn't tell you very much about the distribution of X.  \n",
    "In particular, let's check what happens if $p$ is really 0.55, so that the coin is slightly biased towards heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, we can calculate the power of the test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.113451998705\n",
      "0.886548001295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8865480012953671"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 95% bounds based on the assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "\n",
    "# actual mu and sigma based on p = 0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "\n",
    "# a type 2 error means we fail to reject the null hypothesis, \n",
    "# which will happen when X is still in our original interval\n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(type_2_probability)\n",
    "print(power)\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine instead that our null hypothesis was that the coin is not biased towards heads, or that $p\\le0.5$.  \n",
    "In that case we want a <em>one-sided test</em> that rejects the null hypothesis when X is much larger than 50 but not when X is smaller than 50.  \n",
    "So, let's do a 5%-significance test using <code>normal_probability_below</code> to find the cutoff below which 95% of the probability lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526.0073585242053"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "hi  # is 526 (< 531, since we need more probability in the upper tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363794803307173"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more powerful test, since it no longer rejects $H_0$ when X is below 469 (which is unlikely to happen if $H_1$ is true)  \n",
    "and instead rejects $H_0$ when X is between 526 and 531 (which is somewhat likely to happen if $H_1$ is true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An alternative way of thinking about the preceding test involves p-values.  \n",
    "Instead of choosing bounds based on some probability cutoff, we compute the probability -- assuming $H_0$ is true -- that we would see a value at least as extreme as the one we actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our two-sided test of whether the coin is fair, we compute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598857"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        # if x is greater than the mean, the tail is what's greater than x\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        # if x is less than the mean, the tail is what's less than x\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "\n",
    "# If we were to see 530 heads, we would compute:\n",
    "two_sided_p_value(529.5, mu_0, sigma_0)  # mu_0 is 500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we choose 529.5 instead of 530?  \n",
    "This is called a [continuity correction](https://en.wikipedia.org/wiki/Continuity_correction).  \n",
    "It reflects the fact that <code>normal_probability_between(529.5, 530.5, mu_0, sigma_0)</code> is a better estimate of the probability of seeing 530 heads than <code>normal_probability_between(530, 531, mu_0, sigma_0)</code> is.  \n",
    "Correspondingly, <code>normal_probability_above(529.5, mu_0, sigma_0)</code> is a better estimate of the probability of seeing at least 530 heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to convince yourself that this is a sensible estimate is with a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06155\n"
     ]
    }
   ],
   "source": [
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    # count number of heads in 1000 flips\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 for _ in range(1000))\n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        # and count how often the number is 'extreme'\n",
    "        extreme_value_count += 1\n",
    "        \n",
    "print extreme_value_count / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is greater than our 5% significance, we <em>would not reject</em> the null hypothesis.  \n",
    "If instead we saw 532 heads, the p-value would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is smaller than the 5% significance, which means we <em> would reject</em> the null hypothesis.  \n",
    "It's the exact same test as before, just a different way of approaching the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our one-sided test, if we saw 525 heads we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582083"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(524.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means we <em>would not reject</em> the null hypothesis.  \n",
    "If we saw 527 heads, the computation would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we <em>would reject</em> the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caution\n",
    "Make sure your data is roughly normally distributed before using <code>normal_probability_above()</code> to compute p-values.  \n",
    "The annals of bad data science are filled with examples of people opining that the chance of some observed event occurring at random is one in a million, when what they really mean is \"the chance, assuming the data is distributed normally\", which is pretty meaningless if the data is not distributed normally.  \n",
    "There are various statistical tests for normality, but even plotting the data is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have been testing hypotheses about the value of the heads probability $p$, which is a $parameter$ of the unknown 'heads' distribution.   \n",
    "When this is the case, a third approach is to construct a <em>confidence interval</em> around the observed value of the parameter.  \n",
    "For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip -- 1 if heads, 0 if tails.  \n",
    "If we observe 525 heads out of 1000 flips, then we estimate $p$ equals 0.525.  \n",
    "How confident can we be about this estimate?  \n",
    "Well, if we knew the exact value of $p$, the Central Limit Theorem tells us that the average of those Bernoulli variables should be approximately normal, with mean $p$ and standard deviation <code>math.sqrt(p * (1 - p) / 1000)</code>.  \n",
    "Here, we don't know $p$, so instead we use our estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015791611697353755"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not entirely justified, but people seem to do it anyway. (???)  \n",
    "Using the normal approximation, we conclude that we are \"95 percent confident\" that the following interval contains the parameter $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "This is a statement about the $interval$, not about $p$.  \n",
    "You should understand it as the assertion that if you were to repeat the experiment many times, 95% of the time the \"true\" parameter (which is the same every time) would lie within the observed confidence interval (which might be different every time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we do not conclude that the coin is unfair, since 0.5 falls within our confidence interval.  \n",
    "If instead we had seen 540 heads, then we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma is 0.0157607106439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat \n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print \"Sigma is {}\".format(sigma)\n",
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"fair coin\", doesn't lie in the confidence interval.  \n",
    "In other words, the \"fair coin\" hypothesis doesn't pass a test that you would expect it to pass 95% of the time if it were true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A procedure that erroneously rejects the null hypothesis only 5% of the time will -- by definition -- 5% of the time erroneouly reject the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    \"\"\" Flip a fair coin 1000 times,  True=heads, False=tails\"\"\"\n",
    "    return [random.random() < 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    \"\"\" Using the 5% significance levels \"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_experiment() for _ in range(1000)]\n",
    "num_rejections = len([experiment for experiment in experiments if reject_fairness(experiment)])\n",
    "print num_rejections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all that?  \n",
    "What it means is that if you are setting out to find \"significant\" results, you usually can.  \n",
    "Test enough hypotheses against your data set, and one of them will almost certainly appear significant.  \n",
    "Remove the right outliers, and you can probably get your $p$ value below 0.05.  \n",
    "This is called 'P-hacking' and is in some ways a consequence of the \"inference from p-values framework\" that is discussed [here](http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf).  \n",
    "If you want to do good science, you should:\n",
    "- determine your hypotheses before looking at the data\n",
    "- clean your data without the hypothesis in mind\n",
    "- keep in mind that p-values are not substitutes for common sense  \n",
    "\n",
    "(an alternative is Bayesian Inference, which is discussed a bit later in the chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Running an A/B Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Something from the real world -- trying to get people to click on ads.  \n",
    "Being a scientist, you decide to run an experiment by randomly showing site visitors one of two advertisements and tracking how many people click on each one.  \n",
    "If 990 out of 1000 viewers of ad A click their ad while only 10 out of 1000 viewers of ad B click theirs, you can be pretty confident that A is the better ad.  \n",
    "But what if the differences are not so stark?  \n",
    "Here is where you would use statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that $N_A$ people see ad A and that $n_A$ of them click it.  \n",
    "We can think of each ad view as a Bernoulli trial where $P_A$ is the probability that someone clicks ad A.  \n",
    "Then (if $N_A$ is large, which it is here) we know that $n_A / N_A$ is approximately a a normal random variable with mean $p_A$ and standard deviation $\\large\\sigma_A = {\\sqrt {p_A(1 - p_A) / N_A}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, $n_B/N_B$ is approxiamtely a normal random variable with mean $p_B$ and standard deviation  \n",
    "$\\large\\sigma_B = {\\sqrt{p_B(1 - p_B)/N_B}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimated_parameters(N, n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p) / N)\n",
    "    return p, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume those two normals are independent (which seems reasonable, since the individual Bernoulli trials ought to be), then their difference should also be normal with mean $p_B - p_A$ and standard deviation  \n",
    "$\\large{\\sqrt{\\sigma^2_A + \\sigma^2_B}}$  \n",
    "\n",
    "**Note**  \n",
    "This is sort of cheating.  \n",
    "The math only works out exacly like this if you *know* the standard deviations.  \n",
    "Here, we are estimating them from the data, which means that we really should be using a *t*-distribution.  \n",
    "However, for large enough data sets, it's close enough that it doesn't make much of a difference.  \n",
    "\n",
    "This means that we can test the *null hypothesis* that $P_A$ and $P_B$ are the same thing (that is, $P_A$ and $P_B$ is zero), using the statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which should approximately be a standard normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if ad A gets 200 clicks out of 1000 views and ad B gets 180 clicks out of 1000 views, the statistic equals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1403464899034472"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 180)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of seeing such a large difference if the means were actually equal would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is large enough that you can't conclude that there is much of a difference.  \n",
    "On the other hand, if ad B only got 150 clicks (compared to ad A's 200), we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.948839123097944"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 150)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that there is only a 0.003 probability that you would see such a large difference if the ads were equally effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The procedures we have looked at so far have invloved making probability statements about our tests, for example:  \n",
    "\"There is only a 3% chance that you would observe such an extreme statistic if our null hypothesis were true.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach to inference involves treating the unknown parameters themselves as random variables.  \n",
    "The analyst (that's you) starts with a *prior distribution* for the parameters and then uses the observed data and Bayes' Theorem to get an updated *posterior distibution* for the parameters.  \n",
    "Rather than making probability judgements about the tests, you make probability judgements abiout the parameters themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when the unknown parameter is a probability (as in our coin-flipping example), we often use a prior from the *Beta distribution*, which puts all of its probability between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def B(alpha, beta):\n",
    "    \"\"\" a normalizing constant so that the total probability is 1 \"\"\"\n",
    "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x, alpha, beta):\n",
    "    # no weight outside of [0, 1]\n",
    "    if x < 0 or x > 1:\n",
    "        return 0\n",
    "    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, this distribution centers its weight at:  \n",
    "<code>alpha / (alpha + beta)</code>  \n",
    "and the larger <code>alpha</code> and <code>beta</code> are, the \"tighter\", the distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if <code>alpha</code> and <code>beta</code> are both 1, it's just the uniform distribution (centered at 0.5, very dispersed).  \n",
    "If <code>alpha</code> is much larger than <code>beta</code>, most of the weight is near 1.  \n",
    "If <code>alpha</code> is much smaller than <code>beta</code>, most of the weight is near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's interesting is that this allows us to make probability statements about hypotheses:  \n",
    "\"Based on the prior and the observed data, there is only a 5% likelihood that the coin's heads probability is between 49% and 51%\".  \n",
    "This is philosophically very different from a statement like:  \n",
    "\"If the coin were fair we would expect to observe data so extreme only 5% of the time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "Using Bayesian inference is considered somewhat controversial -- in part because its mathematics can get complicated and in part because of the subjective nature of choosing a prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
