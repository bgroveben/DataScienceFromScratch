{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24. MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that MapReduce is designed to be used on very, very large datasets, so the examples here don't really show the best use cases or its true potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random, re, datetime\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from naive_bayes import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) is a programming model for performing parallel processing on large data sets.  \n",
    "Imagine that we have a (very large) collection that we would like to process in some way.  \n",
    "For example, the items might be website logs, the texts of various books, image files, or anything else.  \n",
    "A basic version of the MapReduce algorithm consists of the following steps:\n",
    "1. Use a `mapper` function to turn each item into zero or more key-value pairs. This is often called the `map()` function, but Python already has a function called `map()`, and we don't want to confuse the two.\n",
    "2. Collect together all of the pairs with identical keys.\n",
    "3. Use a `reducer()` function on each collection of grouped values to produce output values for the corresponding key.  \n",
    "\n",
    "Let's examine this process more concretely using an example that involves counting words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "DataSciencester has grown to millions to users!  \n",
    "This is great for your job security, but it makes routine analyses slightly more difficult.  \n",
    "For example, your VP of Content wants to know what sorts of things people are talking about in their status updates.  \n",
    "As a first attempt, you decide to count the words that appear, so that you can prepare a report on the most frequent ones.  \n",
    "When you only had a few hundred users this was simple to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count_old(documents):\n",
    "    \"\"\" word count not using MapReduce \"\"\"\n",
    "    return Counter(word\n",
    "                   for document in documents\n",
    "                   for word in tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'big': 1, 'data': 2, 'fiction': 1, 'science': 2})"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"data science\", \"big data\", \"science fiction\"]\n",
    "word_count_old(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With millions of users the set of `documents` (status updates) is now too big to fit on your computer.  \n",
    "If you can just fit this into the MapReduce model, you can use some \"big data\" infrastructure that your engineers have implemented.  \n",
    "First, we need a function that turns a document into a sequence of key-value pairs.  \n",
    "We'll want our output to be grouped by word, which means that the keys should be words.  \n",
    "For each word, we'll use the value 1 to indicate that this pair corresponds to one occurrence of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 1),\n",
       " ('data', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('science', 1),\n",
       " ('fiction', 1)]"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wc_mapper(document):\n",
    "    \"\"\" for each word in the document, return (word,1) \"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)\n",
    "# https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python\n",
    "wc_mapper_results = [result\n",
    "                     for document in documents\n",
    "                     for result in wc_mapper(document)]\n",
    "\n",
    "wc_mapper_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the \"plumbing\" step 2 (collect together all the pairs with identical keys) for the moment, imagine that for some word we've collected a list of the corresponding counts we have emitted.  \n",
    "Then to produce the overall count for that word we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wc_reducer(word, counts):\n",
    "    \"\"\" sum up the counts for a word \"\"\"\n",
    "    yield (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now returning to step 2, we need to collect the results from `wc_mapper()` and feed them to `wc_reducer()`.  \n",
    "Let's think about how we can do this on just one computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(documents):\n",
    "    \"\"\" count the words in the input documents using MapReduce \"\"\"\n",
    "    # create a place to store the grouped values\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "       \n",
    "    return [output\n",
    "            for word, counts in collector.iteritems()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take our three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"data science\", \"big data\", \"science fiction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then `wc_mapper` applied to the first document yields the two pairs `(\"data\", 1)` and `(\"science\", 1)`.  \n",
    "After we've gone through all three documents, the `collector` contains:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "defaultdict(<type 'list'>, {'science': [1, 1], 'fiction': [1], 'data': [1, 1], 'big': [1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then `wc_reducer()` produces the count for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 2), ('fiction', 1), ('data', 2), ('big', 1)]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the primary benefit of MapReduce is that it allows us to distribute computations by moving the processing to the data.  \n",
    "Imagine that we want to word-count across billions of documents.  \n",
    "Our original (non-MapReduce) approach requires the machine doing the processing to have access to every document.  \n",
    "This means that the documents all need to either live on that macine or else be transferred to it during processing.  \n",
    "More important, it means that the machine can only process one document at a time.  \n",
    "Granted, you can write code to use multiple processors, multiple cores on the processor, and so on, but the documents still have to get to that machine.  \n",
    "Now, imagine that our billions of documents are scattered across 100 machines.  \n",
    "With the right infrastructure (and glossing over some of the details), we can do the following:  \n",
    "- Have the machine run the mapper on its documents, producing lots of (key, value) pairs.\n",
    "- Distribute those (key, value) pairs to a number of \"reducing\" machines, making sure that the pairs corresponding to any given key all end up on the same machine.\n",
    "- Have each reducing machine group the pairs by key and then run the reduce on each set of values.\n",
    "- Return each (key, output) pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is amazing about this is that it scales horizontally.  \n",
    "If we double the number of machines, then (ignoring certain fixed costs of running a MapReduce system) our calculations will be executed approximately twice as fast.  \n",
    "Each mapper machine will only need to do half as much work, and (assuming that there are enough distinct keys to further distribute the reducer work) the same is true for the reducer machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce More Generally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think about it for a minute, all of the word-count-specific code in the previous example is contained in the `wc_mapper()` and `wc_reducer()` functions.  \n",
    "This means that with a couple of changes we have a much more general framework (that still runs on a single machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\" runs MapReduce on the inputs using mapper and reducer \"\"\"\n",
    "    \n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "            \n",
    "    return [output\n",
    "            for key, values in collector.iteritems()\n",
    "            for output in reducer(key, values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count words simply by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 2), ('fiction', 1), ('data', 2), ('big', 1)]"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = map_reduce(documents, wc_mapper, wc_reducer)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the flexibility to solve a wide variety of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, observe that `wc_reducer()` is just summing the values corresponding to each key.  \n",
    "This kind of aggregation is common enough that it's worth abstracting it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_values_using(aggregation_fn, key, values):\n",
    "    \"\"\" reduces a key-values pair by applying aggregation_fn to the values \"\"\"\n",
    "    yield (key, aggregation_fn(values))\n",
    "    \n",
    "def values_reducer(aggregation_fn):\n",
    "    \"\"\" turns a function (values -> ouput) into a reducer that maps (key, values) -> (key, output) \"\"\"\n",
    "    return partial(reduce_values_using, aggregation_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the following variables each create <functools.partial object at 0xxxxxxxxxx>\n",
    "sum_reducer = values_reducer(sum)\n",
    "max_reducer = values_reducer(max)\n",
    "min_reducer = values_reducer(min)\n",
    "count_distinct_reducer = values_reducer(lambda values: len(set(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Analyzing Status Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content VP was impressed with the word counts and asks what else you can learn from people's status updates.  \n",
    "You manage to extract a data set of status updates that look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_updates = [\n",
    "    { \"id\": 1,\n",
    "      \"username\": \"bgroveben\",\n",
    "      \"text\": \"Yo, imma be geekin my brainz out, howboudah?\",\n",
    "      \"created_at\": datetime.datetime(2017, 2, 15, 3, 30, 0),\n",
    "      \"liked_by\": [\"some_guy\", \"some_gal\", \"cousin_tavo\"] },\n",
    "    { \"id\": 2,\n",
    "      \"username\": \"cousin_tavo\",\n",
    "      \"text\": \"Yo, user bgroveben has very little respect for data science or grammar!\",\n",
    "      \"created_at\": datetime.datetime(2017, 2, 16, 4, 30, 0),\n",
    "      \"liked_by\": [\"some_guy\", \"some_gal\", \"bgroveben\"] },\n",
    "    { \"id\": 3,\n",
    "      \"username\": \"some_gal\",\n",
    "      \"text\": \"Yo, stop worrying about petty things and focus on the data science stuff, people.\",\n",
    "      \"created_at\": datetime.datetime(2017, 2, 17, 5, 15, 0),\n",
    "      \"liked_by\": [\"some_guy\", \"bgroveben\", \"cousin_tavo\"] },\n",
    "    { \"id\": 4,\n",
    "      \"username\": \"some_guy\",\n",
    "      \"text\": \"Big data, big data, data science, data science, data, data.\",\n",
    "      \"created_at\": datetime.datetime(2017, 2, 17, 10, 45, 0),\n",
    "      \"liked_by\": [\"some_gal\", \"bgroveben\", \"cousin_tavo\"] }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we need to figure out which day of the week people talk the most about data science.  \n",
    "In order to find this, we'll just count how many data science updates there are on each day of the week.  \n",
    "This means that we'll need to group our data by day of the week, so that's our key.  \n",
    "If we emit a value of 1 for each update that contains the phrase \"data science\", we can simply get the total number using `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1), (4, 2)]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_science_day_mapper(status_update):\n",
    "    \"\"\" yields (day_of_week, 1) if status_update contains 'data science' \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)  # day_of_week is a number\n",
    "        \n",
    "data_science_days = map_reduce(status_updates,\n",
    "                               data_science_day_mapper,\n",
    "                               sum_reducer)\n",
    "data_science_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a slightly more complicated example, imagine that we need to find out for each user the most common word that she uses in her status updates.  \n",
    "There are three possible approaches that spring to mind for the `mapper`:\n",
    "- Put the username in the key; put the words and counts in the values. \n",
    "- Put the word in the key; put the usernames and counts in the values.\n",
    "- Put the username and word in the key; put the counts in the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think about it a bit more, we definitely want to group by `username`, because we want to consider each person's words separately.  \n",
    "We don't want to group by `word`, since our reducer will need to see all of the words for each person to find out which one is the most popular.  \n",
    "This means that we're going to go with the first option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def words_per_user_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user, (word, 1))\n",
    "        \n",
    "def most_popular_word_reducer(user, words_and_counts):\n",
    "    \"\"\" given a sequence of (word, count) pairs, return the word with the highest total count \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    word, count = word_counts.most_common(1)[0]\n",
    "\n",
    "    yield (user, (word, count))\n",
    "\n",
    "\n",
    "user_words = map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        most_popular_word_reducer)\n",
    "# user_words\n",
    "#!# user_words doesn't return the correct results -- yield in words_per_user_mapper() always emits a count of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could find out the number of distinct status-likers for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('some_guy', 3), ('some_gal', 3), ('bgroveben', 3), ('cousin_tavo', 3)]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def liker_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for liker in status_update[\"liked_by\"]:\n",
    "        yield (user, liker)\n",
    "        \n",
    "distinct_likers_per_user = map_reduce(status_updates,\n",
    "                                      liker_mapper,\n",
    "                                      count_distinct_reducer)\n",
    "distinct_likers_per_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from [Matrix Multiplication](http://localhost:8888/notebooks/21_Chapter_Network_Analysis.ipynb#Matrix-Multiplication) that given an $m \\times n$ matrix $A$ and a $n \\times k$ matrix $B$, we can multiply them to form a $m \\times k$ matrix $C$, where the element of $C$ in row $i$ and column $j$ is given by:  \n",
    "\n",
    "$\\Large C_{ij} = A_{i1}B_{1j} + A_{i2}B_{2j} + \\ldots + A_{in}B_{nj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, a \"natural\" way to represent an $m \\times n$ matrix is with a `list` of `lists`, where the element $A_{ij}$ is the $j$th element of the $i$th list.  \n",
    "However, large matrices are sometimes [sparse](https://en.wikipedia.org/wiki/Sparse_matrix), which means that most of their elements are zero.  \n",
    "For large sparse matrices, a list of lists can be a very wasteful representation.  \n",
    "A more compact representation is a list of tuples (`name`, `i`, `j`, `value`) where `name` identifies the matrix and `i`, `j`, and `value` indicate a location with a nonzero value.  \n",
    "For example, a billion x billion matrix has a [quintillion](https://en.wiktionary.org/wiki/quintillion) entries, which is not easy to store on a computer.  \n",
    "If there are only a few nonzero entries on each row, however, this alternative representation is many orders of magnitude smaller.  \n",
    "Given this sort of tuple representation, it turns out that we can use MapReduce to perform matrix multiplication in a distributed manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate our algorithm, notice that each element $A_{ij}$ is only used to calculate the elements of $C$ in row $i$, and each element $B_{ij}$ is only used to compute thelements of $C$ in column $j$.  \n",
    "Our goal will be for each output of our `reducer` to be a single entry of $C$, which means that we'll need our mapper to emit keys identifying a single entry of $C$.\n",
    "This suggests the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_mapper(m, element):\n",
    "    \"\"\" m is the common dimension (columns of A, rows of B) \"\"\"\n",
    "    \"\"\" element is a tuple (matrix_name, i, j, value) \"\"\"\n",
    "    matrix, i, j, value = element\n",
    "\n",
    "    if matrix == \"A\":\n",
    "        for column in range(m):\n",
    "            # A_ij is the j-th entry in the sum for each C_i_column\n",
    "            yield((i, column), (j, value))\n",
    "    else:\n",
    "        for row in range(m):\n",
    "            # B_ij is the i-th entry in the sum for each C-row_j\n",
    "            yield((row, j), (i, value))\n",
    "\n",
    "\n",
    "def matrix_multiply_reducer(m, key, indexed_values):\n",
    "    results_by_index = defaultdict(list)\n",
    "    for index, value in indexed_values:\n",
    "        results_by_index[index].append(value)\n",
    "    # sum up all of the products of the positions with two results\n",
    "    sum_product = sum(results[0] * results[1]\n",
    "                      for results in results_by_index.values()\n",
    "                      if len(results) == 2)\n",
    "    if sum_product != 0.0:\n",
    "        yield (key, sum_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you had the two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = [[3, 2, 0],\n",
    "     [0, 0, 0]]\n",
    "\n",
    "B = [[4, -1, 0],\n",
    "     [10, 0, 0],\n",
    "     [0,  0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you could rewrite them as tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 1), -3), ((0, 0), 32)]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = [(\"A\", 0, 0, 3), (\"A\", 0, 1,  2),\n",
    "           (\"B\", 0, 0, 4), (\"B\", 0, 1, -1), (\"B\", 1, 0, 10)]\n",
    "\n",
    "mapper = partial(matrix_multiply_mapper, 3)\n",
    "reducer = partial(matrix_multiply_reducer, 3)\n",
    "\n",
    "map_reduce(entries, mapper, reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't terribly interesting on such small matrices, but if you are dealing with millions (or more) of rows and columns, it could help you a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Aside: Combiners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that you have probably noticed is that many of our mappers seem to include a bunch of extra information.  \n",
    "For example, when counting words, rather than emitting `(word, 1)` and summing over the values, we could have emitted `(word, None)` and just taken the length.  \n",
    "One reason we didn't do this is that, in the distributed setting, we sometimes want to use [combiners](https://www.tutorialspoint.com/map_reduce/map_reduce_combiners.htm) to reduce the amount of data that has to be transferred around from machine to machine.  \n",
    "If one of our mapper machines sees the word \"data\" 500 times, we can tell it to combine the 500 instances of `(\"data\", 1)` into a single `(\"data\", 500)` before handing the data off to a reducing machine.  \n",
    "This results in a lot less data getting moved around, which can make our algorithm substantially faster still.  \n",
    "Because of the way we wrote our reducer, it would handle this combined data correctly.  \n",
    "Had we written it using `len()`, it would not have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Further Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
