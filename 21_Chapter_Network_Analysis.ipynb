{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 21. Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random, re\n",
    "from collections import defaultdict, Counter, deque\n",
    "from linear_algebra import dot, get_row, get_column, make_matrix, magnitude, scalar_multiply, shape, distance\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many interesting data problems can be thought of in terms of [networks](https://en.wikipedia.org/wiki/Network_theory) consisting of nodes of some type and the edges that join them (see [graphs](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29) and [graph theory](https://en.wikipedia.org/wiki/Graph_theory) for more information on nodes and edges).  \n",
    "For example, your Facebook friends form the nodes of a network whose edges are friendship relations.  \n",
    "A less obvious example is the World Wide Web itself, with each web page a node, and each hyperlink from one page to another an edge.  \n",
    "Facebook friendship is mutual -- if I am Facebook friends with you then necessarily you are friends with me.  \n",
    "In this case, we say that the edges are [undirected](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29#Undirected_graph).  \n",
    "Hyperlinks are not -- a website $A$ may link to website $Z$ without $Z$ linking to $A$.  \n",
    "We call these types of edges [directed](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29#Directed_graph).  \n",
    "We'll be dealing with both kinds of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 1, we calculated the key connectors in the DataSciencester network by counting the number of friends that each user had.  \n",
    "Now we have enough machinery to look at other approaches.  \n",
    "Recall that the network comprised users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and friendships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what our network looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![users_network_graph](img/users_network_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added friend lists to each user `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "    \n",
    "for i, j in friendships:\n",
    "    # this works because users[i] is the user whose id is i \n",
    "    users[i][\"friends\"].append(users[j])  # add i as a friend of j\n",
    "    users[j][\"friends\"].append(users[i])  # add j as a friend of i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we left off we were dissatisfied with our notion of [degree centrality](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), which didn't really agree with our intuition about who were the key connectors of the network.  \n",
    "An alternative metric is [betweenness centrality](https://en.wikipedia.org/wiki/Centrality#Betweenness_centrality), which identifies people who frequently are on the shortest paths between pairs of other people.  \n",
    "In particular, the betweenness centrality of node $i$ is computed by adding up, for every other pair of nodes $j$ and $k$, the proportion of shortest paths between node $j$ and node $k$ that pass through $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, to figure out Thor's betweenness centrality, we'll need to compute all of the shortest paths between all pairs of people who aren't Thor.  \n",
    "Then we'll need to count how many of those shortest paths pass through Thor.  \n",
    "For instance, the only shortest path between Chi (id 3) and Clive (id 5) passes through Thor, while neither of the shortest paths between Hero (id 0) and Chi (id 3) does.\n",
    "Therefore, as a first step, we'll need to figure out the shortest paths between all pairs of people.  \n",
    "There are some pretty sophisticated algorithms for doing so efficiently, but, because we're learning and building from scratch, we'll use a less efficient but more easily understood algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm, which is an implementation of [breadth-first search](https://en.wikipedia.org/wiki/Breadth-first_search), is one of the more complicated ones in this book, so let's go through it carefully:\n",
    "1. Our goal is a function that takes a `from_user` and finds *all* shortest paths to every other user.\n",
    "2. We'll represent a path as a `list` of user IDs. Since every path starts at `from_user`, we won't include her ID in the list. This means that the length of the list representing the path will be the length of the path itself.\n",
    "3. We'll maintain a dictionary `shortest_paths_to` where the keys are user IDs and the values are lists of paths that end at the user with the specified ID. If there is a unique shortest path, the list will just contain that one path. If there are multiple shortest paths, the list will contain all of them.\n",
    "4. We'll also maintain a [queue](https://en.wikipedia.org/wiki/Breadth-first_search) `frontier` that contains the users we want to explore in the order we want to explore them. (A queue is a data structure that is optimized for operations that \"add to the end\" and \"remove from the front\". In Python, they are implemented as [collections.deque](https://docs.python.org/2/library/collections.html#collections.deque) which is actually a double-ended queue.) We'll store them as pairs (`prev_user, user`) so that we know how we got to each one. We initialize the queue with all of the neighbors of `from_user`.\n",
    "5. As we explore the graph, whenever we find new neighbors that we don't already know the shortest paths to, we add them to the end of the queue to explore later, with the current user as `prev_user`.\n",
    "6. When we take a user off of the queue, and we've never encountered that user before, we've definitely found one or more shortest paths to him -- each of the shortest paths to `prev_user` with one extra step added.\n",
    "7. When we take a user off of the queue and we *have* encountered that user before, then either we've found another shortest path (in which case we should add it) or we've found a longer path (in which case we shouldn't).\n",
    "8. When no more users are left on the queue, we've explored the whole graph (or, at least. the parts of it that are reachable from the starting user) and we're done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can put all of this together into one, rather large, function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shortest_paths_from(from_user):\n",
    "    \n",
    "    # a dictionary from \"user_id\" to *all* shortest paths to that user\n",
    "    shortest_paths_to = { from_user[\"id\"] : [[]] }\n",
    "\n",
    "    # a queue of (previous user, next user) that we need to check.\n",
    "    # starts out with all pairs (from_user, friend_of_from_user)\n",
    "    frontier = deque((from_user, friend)\n",
    "                     for friend in from_user[\"friends\"])\n",
    "\n",
    "    # keep going until we empty the queue\n",
    "    while frontier: \n",
    "\n",
    "        prev_user, user = frontier.popleft() # take from the beginning\n",
    "        user_id = user[\"id\"]\n",
    "\n",
    "        # the fact that we're pulling from our queue means that\n",
    "        # necessarily we already know a shortest path to prev_user\n",
    "        paths_to_prev = shortest_paths_to[prev_user[\"id\"]]\n",
    "        paths_via_prev = [path + [user_id] for path in paths_to_prev]\n",
    "        \n",
    "        # it's possible we already know a shortest path to here as well\n",
    "        old_paths_to_here = shortest_paths_to.get(user_id, [])\n",
    "        \n",
    "        # what's the shortest path to here that we've seen so far?\n",
    "        if old_paths_to_here:\n",
    "            min_path_length = len(old_paths_to_here[0])\n",
    "        else:\n",
    "            min_path_length = float('inf')\n",
    "                \n",
    "        # any new paths to here that aren't too long\n",
    "        new_paths_to_here = [path_via_prev\n",
    "                             for path_via_prev in paths_via_prev\n",
    "                             if len(path_via_prev) <= min_path_length\n",
    "                             and path_via_prev not in old_paths_to_here]\n",
    "        \n",
    "        shortest_paths_to[user_id] = old_paths_to_here + new_paths_to_here\n",
    "        \n",
    "        # add new neighbors to the frontier\n",
    "        frontier.extend((user, friend)\n",
    "                        for friend in user[\"friends\"]\n",
    "                        if friend[\"id\"] not in shortest_paths_to)\n",
    "\n",
    "    return shortest_paths_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can store these `dicts` with each node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user[\"shortest_paths\"] = shortest_paths_from(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to calculate betweenness centrality.  \n",
    "For every pair of nodes $i$ and $j$, we know the $n$ shortest paths from $i$ to $j$.  \n",
    "Then, for each of those paths, we just add $1/n$ to the centrality of each node on that path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "1 3.5\n",
      "2 3.5\n",
      "3 18.0\n",
      "4 20.0\n",
      "5 20.5\n",
      "6 6.0\n",
      "7 6.0\n",
      "8 8.5\n",
      "9 0.0\n"
     ]
    }
   ],
   "source": [
    "for user in users:\n",
    "    user[\"betweenness_centrality\"] = 0.0\n",
    "    \n",
    "for source in users:\n",
    "    source_id = source[\"id\"]\n",
    "    for target_id, paths in source[\"shortest_paths\"].iteritems():\n",
    "        if source_id < target_id:    # don't double count\n",
    "            num_paths = len(paths)   # how many shortest_paths?\n",
    "            contrib = 1 / num_paths  # contribution to centrality\n",
    "            for path in paths:\n",
    "                for id in path:\n",
    "                    if id not in [source_id, target_id]:\n",
    "                        users[id][\"betweenness_centrality\"] += contrib\n",
    "\n",
    "for user in users:\n",
    "    print user[\"id\"], user[\"betweenness_centrality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author included a nice visualization to show the relationships:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![betweenness_centrality](img/betweenness_centrality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, users 0 and 9 have centrality 0.0 (meaning that neither one is on any shortest path between other users), whereas users 3, 4, and 5 all have high centralities (meaning that all three lie on many shortest paths).  \n",
    "Generally the centrality numbers are relative and aren't that meaningful themselves; what we care about is how the numbers for each node compare to the numbers for other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closeness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another measure that we can look at is [closeness centrality](https://en.wikipedia.org/wiki/Centrality#Closeness_centrality).  \n",
    "First, for each user we calculate her [farness](http://www.analytictech.com/ucinet/help/oeky47.htm), which is the sum of the lengths of her shortest paths to each other user.  \n",
    "Since we've already computed the shortest paths between each pair of nodes, we can add up their lengths.  \n",
    "If there are multiple shortest paths, they all have the same length, so we can just look at the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def farness(user):\n",
    "    \"\"\" the sum of the lengths of the shortest paths to each other user \"\"\"\n",
    "    return sum(len(paths[0]) for paths in user[\"shortest_paths\"].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go on and calculate closeness centrality as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0294117647059\n",
      "1 0.037037037037\n",
      "2 0.037037037037\n",
      "3 0.0454545454545\n",
      "4 0.05\n",
      "5 0.05\n",
      "6 0.0416666666667\n",
      "7 0.0416666666667\n",
      "8 0.0357142857143\n",
      "9 0.0277777777778\n"
     ]
    }
   ],
   "source": [
    "for user in users:\n",
    "    user[\"closeness_centrality\"] = 1 / farness(user)\n",
    "    print user[\"id\"], user[\"closeness_centrality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's another pretty picture courtesy of the author to complement our results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![closeness_centrality](img/closeness_centrality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much less variation here -- even the very central nodes are still pretty far from the out on the periphery.  \n",
    "Calculating shortest paths can be a bit of a challenge, which is one reason why betweenness and closeness centrality aren't often used on large networks.  \n",
    "The less intuitive (but generally easier to compute) eigenvector centrality is more frequently used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Time for some math.  \n",
    "In order to talk about [eigenvector centrality](https://en.wikipedia.org/wiki/Eigenvector_centrality), we have to talk about [eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors), and in order to talk about eigenvectors, we have to talk about [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication).  \n",
    "If you need more information, [Khan Academy](https://www.khanacademy.org/) has lessons on ['Eigen-Everything'](https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors) and [Matrix Multiplication](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/composition-of-transformations/v/compositions-of-linear-transformations-1) specifically, and [Linear Algebra](https://www.khanacademy.org/math/linear-algebra) in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A$ is a $n_1 \\times k_1$ matrix and $B$ is a $n_2 \\times k_2$ matrix, and if $k_1 = n_2$, then their product $AB$ is the $n_1 \\times k_2$ matrix whose $(i, j)th$ entry is:  \n",
    "\n",
    "$\\Large A_{i1}B_{1j} + A_{i2}B_{2j} + \\ldots + A_{ik}B_{kj}$\n",
    "\n",
    "which is just the `dot` product of the $i$th row of $A$ (thought of as a vector) with the $j$th column of $B$ (also thought of as a vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_product_entry(A, B, i, j):\n",
    "    return dot(get_row(A, i), get_column(B, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    n1, k1 = shape(A)\n",
    "    n2, k2 = shape(B)\n",
    "    if k1 != n2:\n",
    "        raise ArithmeticError(\"Incompatible Matrix Shapes\")\n",
    "    \n",
    "    return make_matrix(n1, k2, partial(matrix_product_entry, A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if $A$ is a $n \\times k$ matrix and B is a $k \\times 1$ matrix, then $AB$ is a $n \\times 1$ matrix.  \n",
    "If we treat a vector as a one-column matrix, we can think of $A$ as a function that maps $k$-dimensional vectors to $n$-dimensional vectors, where the function is just matrix multiplication.  \n",
    "Previously we represented vectors simply as lists, which isn't quite the same:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v = [1, 2, 3]\n",
    "v_as_matrix = [[1],\n",
    "               [2],\n",
    "               [3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need some helper functions to convert back and forth between the two representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_as_matrix(v):\n",
    "    \"\"\" returns a vector v (represented as a list) as a n x 1 matrix \"\"\"\n",
    "    return [[v_i] for v_i in v]\n",
    "\n",
    "def vector_from_matrix(v_as_matrix):\n",
    "    \"\"\" returns the n x 1 matrix as a list of values \"\"\"\n",
    "    return [row[0] for row in v_as_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after which we can define the matrix operation using `matrix_multiply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_operate(A, v):\n",
    "    v_as_matrix = vector_as_matrix(v)\n",
    "    product = matrix_multiply(A, v_as_matrix)\n",
    "    return vector_from_matrix(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $A$ is a *square* matrix, this operation maps $n$-dimensional vectors to other $n$-dimensional vectors.  \n",
    "It's possible that, for some matrix $A$ and vector $v$, when $A$ operates on $v$ we get back a scalar multiple of $v$.  \n",
    "That is, that the result is a vector that points in the same direction as $v$.  \n",
    "When this happens (and when, in addition, $v$ is not a vector of all zeroes), we call $v$ an *eigenvector* of $A$ and we call the multiplier an *eigenvalue*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible way to find an eigenvector of $A$ is by picking a starting vector $v$, applying `matrix_operate`, rescaling the result to have magnitude 1, and repeating until the process converges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_eigenvector(A, tolerance=0.00001):\n",
    "    guess = [random.random() for __ in A]\n",
    "    \n",
    "    while True:\n",
    "        result = matrix_operate(A, guess)\n",
    "        length = magnitude(result)\n",
    "        next_guess = scalar_multiply(1/length, result)\n",
    "        \n",
    "        if distance(guess, next_guess) < tolerance:\n",
    "            return next_guess, length  # eigenvector, eigenvalue\n",
    "        \n",
    "        guess = next_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, the returned `guess` is a vector such that, when you apply `matrix_operate` to it and rescale it to have length 1, you get back a vector that (almost) perfectly matches itself, which means it's an eigenvector.  \n",
    "Not all matrices of real numbers have eigenvectors and eigenvalues.  \n",
    "For example, the matrix:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rotate = [[ 0, 1],\n",
    "          [-1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rotates vectors 90 degrees clockwise, which means that the only vector it maps to a scalar multiple of itself is a vector of zeroes.  \n",
    "If you tried `find_eigenvector(rotate)` it would run forever.  \n",
    "Even matrices that have eigenvectors can sometimes get stuck in cycles.  \n",
    "Conside the matrix:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flip = [[0, 1],\n",
    "        [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix maps any vector `[x, y]` to `[y, x]`.  \n",
    "This means that, for example, `[1, 1]` is an eigenvector with eigenvalue 1.  \n",
    "However, if you start with a random vector with unequal coordinates, `find_eigenvector` will just repeatedly swap the coordinates forever.  \n",
    "Not-from-scratch libraries like NumPy use different methods that would work in this case.  \n",
    "Nonetheless, when `find_eigenvector` does return a result, that result is indeed an eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neat stuff and all, BUT ... how does this help us understand the DataSciencester network?  \n",
    "To start with, we'll need to represent the connections in our network as an [adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix), whose `(i, j)th` entry is either 1 (if user $i$ and user $j$ are friends) or 0 (if they'r not):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entry_fn(i, j):\n",
    "    return 1 if (i, j) in friendships or (j, i) in friendships else 0\n",
    "\n",
    "n = len(users)\n",
    "adjacency_matrix = make_matrix(n, n, entry_fn)\n",
    "\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector centrality for each user is then the entry corresponding to that user in the eigenvector returned by `find_eigenvector`.  \n",
    "For technical reasons that are way beyond the scope of this book, any nonzero adjacency matrix necessarily has an eigenvector all of whose values are non-negative.  \n",
    "Fortunately for us, for this `adjacency_matrix` our `find_eigenvector` function finds it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3857811601502742,\n",
       " 0.514789978225433,\n",
       " 0.514789978225433,\n",
       " 0.4733141614761959,\n",
       " 0.23360705443299834,\n",
       " 0.1501484482979286,\n",
       " 0.08355071695942651,\n",
       " 0.08355071695942651,\n",
       " 0.07284250922111579,\n",
       " 0.0272922972810267]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvector_centralities, _ = find_eigenvector(adjacency_matrix)\n",
    "\n",
    "eigenvector_centralities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another visual aid courtesy of the author:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eigenvector_centrality](img/eigenvector_centrality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users with high eigenvector centrality should be those who have a lot of connections as well as connections to people who themselves have high centrality.  \n",
    "As shown above, users 1 and 2 are the most central, as they both have three connections to people who are themselves highly central.  \n",
    "As we move away from them, people's centralities steadily drop off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a network this small, eigenvector centrality behaves somewhat erratically.  \n",
    "If you try adding or subtracting links, you'll find that small changes in the network can dramatically change the centrality numbers.  \n",
    "In a much larger network this would not be the case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still haven't explained why an eigenvector might lead to a reasonable notion of centrality.  \n",
    "Being an eigenvector means that if you calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.029579956450866,\n",
       " 1.373885299851903,\n",
       " 1.373885299851903,\n",
       " 1.2631870108838643,\n",
       " 0.6234626097741245,\n",
       " 0.40070848835185136,\n",
       " 0.22299095751904438,\n",
       " 0.22299095751904438,\n",
       " 0.1943937311998797,\n",
       " 0.07284250922111579]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_operate(adjacency_matrix, eigenvector_centralities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result is a [scalar multiple](http://www.mathnotes.org/?pid=87#?pid=87) of `eigenvector_centralities`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at how matrix multiplication works, `matrix_operate` produces a vector whose $i$th element is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1943937311998797"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot(get_row(adjacency_matrix, i), eigenvector_centralities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is precisely the sum of the eigenvector centralities of the users connected to user $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, eigenvector centralities are numbers, one per user, such that each user's value is a constant multiple of the sum of her neighbors' values.  \n",
    "In this case centrality means being connected to people who themselves are central.  \n",
    "The more centrality you are directly connected to, the more central you are.  \n",
    "This is, of course, a circular definition, and eigenvectors are the way of breaking out of the circularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of understanding this is by thinking about what `find_eigenvector` is doing here.  \n",
    "It starts by assigning each node a random centrality.  \n",
    "It then repeats the following two steps until the process converges on a result:  \n",
    "1. Give each node a new centrality score that equals the sum of its neighbors' (old) centrality scores.\n",
    "2. Rescale the vector of centralities to have magnitude 1.  \n",
    "\n",
    "Althought the math behind this method may seem somewhat opaque at first, the calculation itself is relatively straighforward (compared to betweenness centrality, anyway) and is fairly simple to perform on even very large graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Graphs and PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSciencester isn't getting much traction, so the VP of Revenue considers pivoting from a friendship model to an endorsement model.  \n",
    "It turns out that no one really cares which data scientists are *friends* with one another, but tech recruiters care very much which data scientists are respected by other data scientists.  \n",
    "In this new model, we'll track endorsements `(source, target)` that no longer represent a reciprocal relationship, but rather that `source` endorses `target` as an awesome data scientist:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![network_of_endorsements](img/network_of_endorsements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first need to account for this asymmetry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2),\n",
    "                (2, 1), (1, 3), (2, 3), (3, 4), (5, 4),\n",
    "                (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\n",
    "\n",
    "for user in users:\n",
    "    user[\"endorses\"] = []     # add one list to track outgoing endorsements \n",
    "    user[\"endorsed_by\"] = []  # and another to track endorsements\n",
    "    \n",
    "for source_id, target_id in endorsements:\n",
    "    users[source_id][\"endorses\"].append(users[target_id])\n",
    "    users[target_id][\"endorsed_by\"].append(users[target_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after which we can find the `most_endorsed` data scientists and sell that information to recruiters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endorsements_by_id = [(user[\"id\"], \n",
    "                       len(user[\"endorsed_by\"])) \n",
    "                       for user in users]\n",
    "\n",
    "sorted(endorsements_by_id, key=lambda (user_id, num_endorsements): num_endorsements, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that 'number of endorsements' is an easy metric to game.  \n",
    "All someone needs to do to manipulate the rankings is create fake accounts and have them endorse someone, or arrange with your friends to endorse each other (as users 0, 1, and 2 appear to have done).  \n",
    "A better metric would be to take into account *who* endorses you.  \n",
    "Endorsements from people who have a lot of endorsements themselves should (somehow) carry more weight than endorsements from people who only have a few of them.  \n",
    "This is the essence of the [PageRank algorithm](https://en.wikipedia.org/wiki/PageRank), which is used by Google to rank websites based on which other websites link to them, which other websites link to those, and so on.  \n",
    "If this reminds you of the idea behind eigenvector centrality, it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simplified version of PageRank works like this:\n",
    "1. There is a total of 1.0 (or 100%) PageRank in the network.\n",
    "2. Initially this PageRank is equally distributed among nodes.\n",
    "3. At each step, a large fraction of each node's PageRank is distributed evenly among its outgoing links.\n",
    "4. At each step, the remainder of each node's PageRank is distributed evenly among all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0404553415061296,\n",
       " 1: 0.044921190893169885,\n",
       " 2: 0.044921190893169885,\n",
       " 3: 0.0404553415061296,\n",
       " 4: 0.06785083675770529,\n",
       " 5: 0.04344422700587085,\n",
       " 6: 0.03346379647749512,\n",
       " 7: 0.03346379647749512,\n",
       " 8: 0.04344422700587085,\n",
       " 9: 0.03346379647749512}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def page_rank(users, damping=0.85, num_iters=100):\n",
    "    \n",
    "    # initially distribute PageRank evenly \n",
    "    num_users = len(users)\n",
    "    pr = { user[\"id\"] : 1 / num_users for user in users }\n",
    "    \n",
    "    # this is the small fraction of PageRank that each node gets in each iteration\n",
    "    base_pr = (1 - damping) / num_users\n",
    "    \n",
    "    for __ in range(num_iters):\n",
    "        next_pr = { user[\"id\"] : base_pr for user in users }\n",
    "        for user in users:\n",
    "            # distribute PageRank to outgoing links\n",
    "            links_pr = pr[user[\"id\"]] * damping\n",
    "            for endorsee in user[\"endorses\"]:\n",
    "                next_pr[endorsee[\"id\"]] += links_pr / len(user[\"endorses\"])\n",
    "        pr = next_pr\n",
    "                \n",
    "    return pr\n",
    "\n",
    "page_rank(users, damping=0.85, num_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our version of PageRank, user 4 (Thor) is the highest ranked data scientist.  \n",
    "Again with the visualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![page_rank](img/page_rank.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though Thor has fewer endorsements (2) than users 0, 1, and 2, his endorsements carry with them the weight of rank from their other endorsements.  \n",
    "Furthermore, both of Thor's endorsers endorsed *only* him, which means that he doesn't have to divide their rank with anyone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Further Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
