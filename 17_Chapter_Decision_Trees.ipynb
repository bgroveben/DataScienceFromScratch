{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSciencester's VP of Talent has interviewed a number of job candidates from the site, with varying degrees of success.  \n",
    "He's collected a data set consisting of several (qualitative) attributes of each candidate, as well as whether that candidate interviewed well or poorly.  \n",
    "Could you, he asks, use this data to build a model identifying which candidates will interview well, so that he doesn't have to waste time conducting interviews?  \n",
    "This task seems like a good fit for a [decision tree](https://en.wikipedia.org/wiki/Decision_tree), which is another predictive modeling tool in the data scientist's kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree uses a tree structure to represent a number of possible [decision paths](http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html) and an outcome for each path.  \n",
    "If you have ever played the game [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_Questions), then you are familiar with decision trees and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have a lot to recommend them.  \n",
    "They are very easy to understand and interpret, and the process by which they reach a prediction is very transparent.  \n",
    "Decision trees can easily handle a mix of numeric and categorical attributes as well as classify data for which attributes are missing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, finding an \"optimal\" decision tree for a set of training data is computationally a very hard problem.  \n",
    "We will work around this problem by trying to build a good-enough tree rather than an optimal one, although for large data sets this can still be a lot of work.  \n",
    "More importantly, it is very easy (and very bad) to build decision trees that are [overfitted](https://en.wikipedia.org/wiki/Overfitting) to the training data, and that don't generalize well to unseen data.  \n",
    "We'll look at ways to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are often divided into [classification trees](http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf), which produce categorical outputs, and [regression trees](http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf), which produce numeric outputs.  \n",
    "In this chapter, we will focus on classification trees.  \n",
    "We'll work through the [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm) for learning a decision tree from a set of labeled data, which should help us understand how decision trees actually work.  \n",
    "To make things simple, we'll restrict ourselves to problems with binary outputs like, \"Should I hire this candidate?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to build a decision tree, we will need to decide what questions to ask and in what order.  \n",
    "At each stage of the tree there are some possibilities that we have eliminated and some that we haven't.  \n",
    "After learning that an animal doesn't have more than five legs, we've eliminated the possibility that it's a grasshopper, but we haven't eliminated the possibility that it's a duck.  \n",
    "Every possible question partitions the remaining possibilities according to their answers.  \n",
    "Ideally, we would like to choose questions whose answers give a lot of information about what our tree should predict.  \n",
    "If there is a single yes/no question for which \"yes\" answers always correspond to `True` outputs and \"no\" answers to `False` outputs (or vice versa), this would be an awesome question to pick.\n",
    "We capture this notion of \"how much information\" with [entropy](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution), a term that we will use to represent the uncertainty associated with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a set $S$ of data, each member of which is labeled as belonging to one of a finite number of classes $C_1,\\;\\ldots,\\;C_n$.  \n",
    "If all of the data points belog to a single class, then there is no real uncertainty, which means we'd like there to be low entropy.  \n",
    "If the data points are evenly spread across the classes, then there is a lot of uncertainty, and we'd like there to be high entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In math terms, if $p_i$ is the proportion of data labeled as class $C_i$, we define the entropy as:  \n",
    "\n",
    "$\\Large H(S) = -\\;p_1\\text{log}_2p_1 - \\;\\ldots\\; - p_n\\text{log}_2p_n$  \n",
    "\n",
    "with the (standard) convention that $0\\;\\text{log}\\;0 = 0$.  \n",
    "The following plot should aid your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGHCAYAAACJeOnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1084d1490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(0.0001, 1.0, 1000)\n",
    "# xs = np.arange(0.001, 1.0, 0.001)  also works\n",
    "ys = -xs * np.log2(xs)\n",
    "plt.title(\"A Graph Showing -p log(p)\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"-p log(p)\")\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without worrying too much about the details, each term $-p_i\\text{log}_2p_i$ is non-negative and is close to zero precisely when $p_i$ is either close to zero or close to one.  \n",
    "This means that the entropy will be small when every $p_i$ is close to 0 or 1 (like when most of the data is in a single class), and it will be larger when many of the $p_i$'s are not too close to 0 (like when the data is spread across multiple classes).  \n",
    "This is exactly the behavior that we desire, and it is simple enough to roll all of this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\" given a list of class probabilities, calculate the entropy \"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)  # if p means ignore zero probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will consist of pairs (`input`, `label`), which means that we will need to compute the class probabilities ourselves.  \n",
    "Observe that we don't actually care which label is associated with each probability, only what the probabilites are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Entropy of a Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What we have done so far is calculate the entropy (\"uncertainty\") of a single set of labeled data.  \n",
    "Each stage of a decision tree involves asking a question whose answer partitions data into one or (hopefully) more subsets.  \n",
    "As an example, our \"does it have more than five legs?\" question partitions animals into those that have more than 5 legs and those that don't.  \n",
    "We would also like some idea of the entropy that results from partitioning a set of data in a certain way.  \n",
    "We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (high certainty), and high entropy if the data contains subsets that are large and have low certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, if we partition our data $S$ into subsets $S_1,\\;\\ldots,\\;S_m$ containing proportions $q_1,\\;\\ldots,\\;q_m$ of the data, then we can calculate the entropy of the partition as a weighted sum:  \n",
    "\n",
    "$\\Large H = q_1H(S_1) + \\;\\ldots\\;+ q_mH(S_m)$  \n",
    "\n",
    "which can be implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\" find the entropy from this partition of data into subsets \"\"\"\n",
    "    \"\"\" subsets is a list of lists of labeled data \"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "One problem with this approach is that partitioning by an attribute with many different values will result in a very low entropy due to overfitting.  \n",
    "For example, imagine that you work for a bank and you are trying to build a decision tree to predict which of your customers are likely to default on their mortgages, using some historical data as your training set.  \n",
    "This example data contains each customer's Social Security Number.  \n",
    "Partitioning on SSN will produce one-person subsets, each of which has zero entropy.  \n",
    "However, a model that relies on SSN will not generalize beyond the training set.  \n",
    "For this reason, try to avoid (or at least bucket, if appropriate) attributes with large numbers of possible values when creating decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The VP provides you with the interviewee data, consisting of pairs (`input`, `label`), where each `input` is a `dict` of candidate attributes, and each label is either `True` (the candidate interviewed well) or `False` (the candidate interviewed poorly).  \n",
    "In particular, you are provided with each candidate's level, her preferred language, whether she is active on Twitter, and whether she has a PhD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "    ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "    ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "    ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "    ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "    ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "    ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "    ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "    ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "    ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "    ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "    ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "    ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "    ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tree will consist of *decision nodes*, which ask us a question and direct us differently depending on the answer, and *leaf nodes*, which give us a prediction.  \n",
    "We will build it using the relatively simple [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm) (yes, I linked it again), which operates in the following manner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we're given some labeled data, and a list of attributes to consider branching on.  \n",
    "- If the data all have the same label, then create a leaf node that predicts that label and then stop.\n",
    "- If the list of attributes is empty (meaning there are no more possible questions to ask), then create a leaf node that predicts the most common label and stop.\n",
    "- Otherwise, try partitioning the data by each of the attributes.\n",
    "- Choose the partition with the lowest partition entropy.\n",
    "- Add a decision noode based on the chosen attribute.\n",
    "- Recur on each partitioned subset using the remaining attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what's known as a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm), because at each step it makes the locally optimal choice (first best option).  \n",
    "Given a data set, there may be a better tree with a worse-looking first move.  \n",
    "If so, this algorithm won't find it.  \n",
    "Nonetheless, ID3 is relatively easy to understand and implement, which makes it a good place to begin exploring decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually go through these steps on the interviewee data set.  \n",
    "The data set has bot `True` and `False` labels, and we have four attributes that we can split on.  \n",
    "Our first step will be to find the partition with the least entropy.  \n",
    "We'll start by writing a function that does the partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    \"\"\" each input is a pair (attribute_dict, label) \"\"\"\n",
    "    \"\"\" returns a dict : attribute_value -> inputs \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = input[0][attribute]   # get the value of the specified attribute\n",
    "        groups[key].append(input)   # then add this input to the correct list\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as a function that uses it to calculate entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\" calculates the entropy corresponding to the given partition \"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to find the minimum-entropy partition for the whole data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.693536138896\n",
      "lang 0.860131712855\n",
      "tweets 0.788450457308\n",
      "phd 0.892158928262\n"
     ]
    }
   ],
   "source": [
    "for key in ['level', 'lang', 'tweets', 'phd']:\n",
    "    print key, partition_entropy_by(inputs, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest entropy comes from splitting on `level`, so we'll need to make a subtree for each possible `level` value.  \n",
    "Every `Mid` candidate is labeled `True`, which means that the `Mid` subtree is simply a leaf node predicting `True`.  \n",
    "For `Senior` candidiates, we have a mix of `True`s and `False`s, so we need to split again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.950977500433\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [(input, label) for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print key, partition_entropy_by(senior_inputs, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show us that our next split should be on `tweets`, which results in a zero-entropy partition.  \n",
    "For these Senior-level candidates, \"yes\" on `tweets` always result in `True`, while \"no\" on `tweets` always result in `False`.  \n",
    "Finally, if we do the same thing for the `Junior` candidates, we end up splitting on `phd`, after which we find that no PhD always results in `True` and (yes on) PhD always results in `False`.  \n",
    "The following easy-to-read visualization shows the complete decision tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decision_tree](img/decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we've seen how the algorithm works, we would like to implement it more generally.  \n",
    "This means that we need to decide how we want to represent trees.  \n",
    "Let's keep things simple and define *tree* as one of the following:  \n",
    "- `True`\n",
    "- `False`\n",
    "- a tuple (`attribute`, `subtree_dict`)  \n",
    "\n",
    "Here `True` represents a leaf node that returns `True` for any input, `False` represents a leaf node that returns `False` for any input, and a tuple represents a decision node that, for any input, finds it `attribute` value, and classifies the input using the corresponding subtree.  \n",
    "\n",
    "With this representation, our hiring tree would look like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "('level',\n",
    "  {'Junior': ('phd', {'no': True, 'yes': False}),\n",
    "   'Mid': True,\n",
    "   'Senior': ('tweets', {'no': False, 'yes': True})})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still the question of what to do if we encounter an unexpected (or missing) attribute value.  \n",
    "What should our hiring tree do if it encounters a candidate whose `level` is \"Intern\"?  \n",
    "We can handle this case by adding a `None` key that predicts the most common label (although this is a bad idea if `None` is actually a that appears in the data).  \n",
    "Given such a representation, we can classify an input with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\" classify the input using the given decision tree \"\"\"\n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "    # otherwise this tree consists of an attribute to split on and a dictionary whose keys are \n",
    "    # values of that attribute and whose values are the subtrees to consider next\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute)   # None if input is missing the attribute\n",
    "    \n",
    "    if subtree_key not in subtree_dict:  # if there is no subtree for key,\n",
    "        subtree_key = None               # use the None subtree\n",
    "        \n",
    "    subtree = subtree_dict[subtree_key]  # choose the appropriate subtree,\n",
    "    return classify(subtree, input)      # and use it to classify the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to build the tree representation from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "    # if this is our first pass, all keys of the first input are split_candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "        \n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0: return False     # if no 'Trues' then return a 'False' leaf\n",
    "    if num_falses == 0: return True     # if no 'Falses' then return a 'True' leaf\n",
    "    if not split_candidates:            # if there are no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "    \n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates, key=partial(partition_entropy_by, inputs))\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates if a != best_attribute]\n",
    "    \n",
    "    # and recursively build the subtrees\n",
    "    subtrees = { attribute_value : build_tree_id3(subset, new_candidates) \n",
    "                for attribute_value, subset in partitions.iteritems() }\n",
    "    subtrees[None] = num_trues > num_falses  # default case\n",
    "    \n",
    "    return (best_attribute, subtrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tree that we built, every leaf consisted entirely of `True` inputs or entirely of `False` inputs.  \n",
    "This means that the tree predicts perfectly on the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('level',\n",
       " {None: True,\n",
       "  'Junior': ('phd', {None: True, 'no': True, 'yes': False}),\n",
       "  'Mid': True,\n",
       "  'Senior': ('tweets', {None: False, 'no': False, 'yes': True})})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = build_tree_id3(inputs)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also apply our model to new data that was not in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                 \"lang\" : \"Java\",\n",
    "                 \"tweets\" : \"yes\",\n",
    "                 \"phd\" : \"no\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                 \"lang\" : \"Java\",\n",
    "                 \"tweets\" : \"yes\",\n",
    "                 \"phd\" : \"yes\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as data with missing or unexpected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Intern\" } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Senior\" } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "Since our goal was mainly to demonstrate *how* to build a tree, we built the tree using the entire data set.  \n",
    "As always, if we were really trying to create a good model for something, we would have collected more data and split it into train/validation/test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given how closely decision trees can fit themselves to their training data, it's not surprising that they have a tendency to overfit.  \n",
    "One way of avoiding this is a technique called [random forests](https://en.wikipedia.org/wiki/Random_forest), in which we build multiple decision trees and let them 'vote' on how to classify inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tree-building process was deterministic, so how do we get random trees?  \n",
    "One piece involves bootstrapping data (see [Digression: The Bootstrap](http://localhost:8888/notebooks/15_Chapter_Multiple_Regression.ipynb#Digression:-The-Bootstrap)).  \n",
    "Rather than training each tree on all of the `inputs` in the training set, we instead train each tree on the result of `bootstrap_sample(inputs)`.  \n",
    "Since each tree is built using different data, each tree will be different from every other tree.  \n",
    "One side benefit of this practice is that it's totally fair to use the nonsampled data to test each tree, which means that you can get away with using all of your data as the training set if you are careful (and clever) about how you measure performance.  \n",
    "This technique is known as [*bootstrap aggregating* or *bagging*](https://en.wikipedia.org/wiki/Bootstrap_aggregating)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second source of randomness involves changing the way we chose the `best_attribute` to split on.  \n",
    "Rather than looking at all of the remaining attributes, we first choose a random subset of them and then split on whichever of those is best:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if there are few enough split candidates, look at all of them\n",
    "\n",
    "if len(split_candidates) <= self.num_split_candidates:\n",
    "    sampled_split_candidates = split_candidates\n",
    "    \n",
    "# otherwise pick a random sample\n",
    "\n",
    "else:\n",
    "    sampled_split_candidates = random.sample(split_candidates, self.num_split_candidates)\n",
    "\n",
    "# now choose the best attribute only from those candidates\n",
    "\n",
    "best_attribute = min(sampled_split_candidates, key=partial(partition_entropy_by, inputs))\n",
    "partitions = partition_by(inputs, best_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a broader technique called [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) in which we combine several [weak learners](https://stats.stackexchange.com/questions/82049/what-is-meant-by-weak-learner) (typically high-bias, low-variance models) in order to produce an overall strong model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn has many [Decision Tree models](http://scikit-learn.org/stable/modules/tree.html).  \n",
    "It also has an [ensemble module](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) that includes a `RandomForestClassifier` as well as other ensemble methods.  \n",
    "We barely scratched the surface of decision trees and their algorithms.  \n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning) is a good starting point for further exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
