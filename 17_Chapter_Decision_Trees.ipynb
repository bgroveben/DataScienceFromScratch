{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSciencester's VP of Talent has interviewed a number of job candidates from the site, with varying degrees of success.  \n",
    "He's collected a data set consisting of several (qualitative) attributes of each candidate, as well as whether that candidate interviewed well or poorly.  \n",
    "Could you, he asks, use this data to build a model identifying which candidates will interview well, so that he doesn't have to waste time conducting interviews?  \n",
    "This task seems like a good fit for a [decision tree](https://en.wikipedia.org/wiki/Decision_tree), which is another predictive modeling tool in the data scientist's kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree uses a tree structure to represent a number of possible [decision paths](http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html) and an outcome for each path.  \n",
    "If you have ever played the game [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_Questions), then you are familiar with decision trees and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have a lot to recommend them.  \n",
    "They are very easy to understand and interpret, and the process by which they reach a prediction is very transparent.  \n",
    "Decision trees can easily handle a mix of numeric and categorical attributes as well as classify data for which attributes are missing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, finding an \"optimal\" decision tree for a set of training data is computationally a very hard problem.  \n",
    "We will work around this problem by trying to build a good-enough tree rather than an optimal one, although for large data sets this can still be a lot of work.  \n",
    "More importantly, it is very easy (and very bad) to build decision trees that are [overfitted](https://en.wikipedia.org/wiki/Overfitting) to the training data, and that don't generalize well to unseen data.  \n",
    "We'll look at ways to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are often divided into [classification trees](http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf), which produce categorical outputs, and [regression trees](http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf), which produce numeric outputs.  \n",
    "In this chapter, we will focus on classification trees.  \n",
    "We'll work through the [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm) for learning a decision tree from a set of labeled data, which should help us understand how decision trees actually work.  \n",
    "To make things simple, we'll restrict ourselves to problems with binary outputs like, \"Should I hire this candidate?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to build a decision tree, we will need to decide what questions to ask and in what order.  \n",
    "At each stage of the tree there are some possibilities that we have eliminated and some that we haven't.  \n",
    "After learning that an animal doesn't have more than five legs, we've eliminated the possibility that it's a grasshopper, but we haven't eliminated the possibility that it's a duck.  \n",
    "Every possible question partitions the remaining possibilities according to their answers.  \n",
    "Ideally, we would like to choose questions whose answers give a lot of information about what our tree should predict.  \n",
    "If there is a single yes/no question for which \"yes\" answers always correspond to `True` outputs and \"no\" answers to `False` outputs (or vice versa), this would be an awesome question to pick.\n",
    "We capture this notion of \"how much information\" with [entropy](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution), a term that we will use to represent the uncertainty associated with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a set $S$ of data, each member of which is labeled as belonging to one of a finite number of classes $C_1,\\;\\ldots,\\;C_n$.  \n",
    "If all of the data points belog to a single class, then there is no real uncertainty, which means we'd like there to be low entropy.  \n",
    "If the data points are evenly spread across the classes, then there is a lot of uncertainty, and we'd like there to be high entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In math terms, if $p_i$ is the proportion of data labeled as class $C_i$, we define the entropy as:  \n",
    "\n",
    "$\\Large H(S) = -\\;p_1\\text{log}_2p_1 - \\;\\ldots\\; - p_n\\text{log}_2p_n$  \n",
    "\n",
    "with the (standard) convention that $0\\;\\text{log}\\;0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without worrying too much about the details, each term $-p_i\\text{log}_2p_i$ is non-negative and is close to zero precisely when $p_i$ is either close to zero or close to one.  \n",
    "This means that the entropy will be small when every $p_i$ is close to 0 or 1 (like when most of the data is in a single class), and it will be larger when many of the $p_i$'s are not too close to 0 (like when the data is spread across multiple classes).  \n",
    "This is exactly the behavior that we desire, and it is simple enough to roll all of this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\" given a list of class probabilities, calculate the entropy \"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)  # if p means ignore zero probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will consist of pairs (`input`, `label`), which means that we will need to compute the class probabilities ourselves.  \n",
    "Observe that we don't actually care which label is associated with each probability, only what the probabilites are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Entropy of a Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What we have done so far is calculate the entropy (\"uncertainty\") of a single set of labeled data.  \n",
    "Each stage of a decision tree involves asking a question whose answer partitions data into one or (hopefully) more subsets.  \n",
    "As an example, our \"does it have more than five legs?\" question partitions animals into those that have more than 5 legs and those that don't.  \n",
    "We would also like some idea of the entropy that results from partitioning a set of data in a certain way.  \n",
    "We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (high certainty), and high entropy if the data contains subsets that are large and have low certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, if we partition our data $S$ into subsets $S_1,\\;\\ldots,\\;S_m$ containing proportions $q_1,\\;\\ldots,\\;q_m$ of the data, then we can calculate the entropy of the partition as a weighted sum:  \n",
    "\n",
    "$\\Large H = q_1H(S_1) + \\;\\ldots\\;+ q_mH(S_m)$  \n",
    "\n",
    "which can be implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\" find the entropy from this partition of data into subsets \"\"\"\n",
    "    \"\"\" subsets is a list of lists of labeled data \"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "One problem with this approach is that partitioning by an attribute with many different values will result in a very low entropy due to overfitting.  \n",
    "For example, imagine that you work for a bank and you are trying to build a decision tree to predict which of your customers are likely to default on their mortgages, using some historical data as your training set.  \n",
    "This example data contains each customer's Social Security Number.  \n",
    "Partitioning on SSN will produce one-person subsets, each of which has zero entropy.  \n",
    "However, a model that relies on SSN will not generalize beyond the training set.  \n",
    "For this reason, try to avoid (or at least bucket, if appropriate) attributes with large numbers of possible values when creating decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
